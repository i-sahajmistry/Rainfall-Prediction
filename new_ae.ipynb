{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, ReLU\n",
    "from keras import layers, Sequential\n",
    "from tensorflow import matmul, Variable\n",
    "from tensorflow import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data\n",
    "- Data is of Rainfall in months of June to September of years 1948 - 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "\n",
    "rain = pd.read_csv(\"./Data/Rainfall.csv\")\n",
    "\n",
    "lpa = statistics.mean(list(rain['Jun-Sep'][12:41]))\n",
    "rain_fall_data = list(rain['Jun-Sep'])\n",
    "\n",
    "rain_data = []\n",
    "for i in range(len(rain_fall_data)):\n",
    "  temp = (rain_fall_data[i]/lpa)*100\n",
    "  rain_data.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Importing Sea-Level Pressure(SLP), Sea Surface Temperature(SST), and Zonal Wind(UWND) Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "slp_path  = \"./Data/Anomaly/slp/*\"\n",
    "sst_path  = \"./Data/Anomaly/sst/*\"\n",
    "uwnd_path = \"./Data/Anomaly/uwnd/*\"\n",
    "\n",
    "slp_months = []\n",
    "for i in glob.glob(slp_path):\n",
    "  slp_months.append(i)\n",
    "\n",
    "sst_months = []\n",
    "for i in glob.glob(sst_path):\n",
    "  sst_months.append(i)\n",
    "\n",
    "uwnd_months = []\n",
    "for i in glob.glob(uwnd_path):\n",
    "  uwnd_months.append(i)\n",
    "  \n",
    "slp_months.sort()\n",
    "sst_months.sort()\n",
    "uwnd_months.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Start and End Index for data (i.e 1948's index and 2000's index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "e = 52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Extracting data of all the months and stacking one above other. \n",
    "- 324 variables and 52 data points for SLP and UWND.\n",
    "- 192 variables and 52 data points for SST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Latitude</th>\n",
       "      <th colspan=\"10\" halign=\"left\">-90</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">80</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Longitude</th>\n",
       "      <th>0</th>\n",
       "      <th>20</th>\n",
       "      <th>40</th>\n",
       "      <th>60</th>\n",
       "      <th>80</th>\n",
       "      <th>100</th>\n",
       "      <th>120</th>\n",
       "      <th>140</th>\n",
       "      <th>160</th>\n",
       "      <th>180</th>\n",
       "      <th>...</th>\n",
       "      <th>160</th>\n",
       "      <th>180</th>\n",
       "      <th>200</th>\n",
       "      <th>220</th>\n",
       "      <th>240</th>\n",
       "      <th>260</th>\n",
       "      <th>280</th>\n",
       "      <th>300</th>\n",
       "      <th>320</th>\n",
       "      <th>340</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1949</th>\n",
       "      <td>-3.649323</td>\n",
       "      <td>-3.649323</td>\n",
       "      <td>-3.649323</td>\n",
       "      <td>-3.649323</td>\n",
       "      <td>-3.649323</td>\n",
       "      <td>-3.649323</td>\n",
       "      <td>-3.649323</td>\n",
       "      <td>-3.649323</td>\n",
       "      <td>-3.649323</td>\n",
       "      <td>-3.649323</td>\n",
       "      <td>...</td>\n",
       "      <td>6.598676</td>\n",
       "      <td>5.294682</td>\n",
       "      <td>3.563006</td>\n",
       "      <td>1.995687</td>\n",
       "      <td>1.005682</td>\n",
       "      <td>0.951014</td>\n",
       "      <td>1.386347</td>\n",
       "      <td>1.43035</td>\n",
       "      <td>1.857351</td>\n",
       "      <td>3.877972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1950</th>\n",
       "      <td>1.230677</td>\n",
       "      <td>1.230677</td>\n",
       "      <td>1.230677</td>\n",
       "      <td>1.230677</td>\n",
       "      <td>1.230677</td>\n",
       "      <td>1.230677</td>\n",
       "      <td>1.230677</td>\n",
       "      <td>1.230677</td>\n",
       "      <td>1.230677</td>\n",
       "      <td>1.230677</td>\n",
       "      <td>...</td>\n",
       "      <td>-4.251374</td>\n",
       "      <td>-4.835348</td>\n",
       "      <td>-5.186994</td>\n",
       "      <td>-4.854313</td>\n",
       "      <td>-2.824318</td>\n",
       "      <td>-0.829026</td>\n",
       "      <td>-1.153683</td>\n",
       "      <td>-1.92965</td>\n",
       "      <td>-1.202699</td>\n",
       "      <td>0.617962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>7.090677</td>\n",
       "      <td>7.090677</td>\n",
       "      <td>7.090677</td>\n",
       "      <td>7.090677</td>\n",
       "      <td>7.090677</td>\n",
       "      <td>7.090677</td>\n",
       "      <td>7.090677</td>\n",
       "      <td>7.090677</td>\n",
       "      <td>7.090677</td>\n",
       "      <td>7.090677</td>\n",
       "      <td>...</td>\n",
       "      <td>0.478676</td>\n",
       "      <td>3.114682</td>\n",
       "      <td>5.042976</td>\n",
       "      <td>5.235657</td>\n",
       "      <td>5.285642</td>\n",
       "      <td>5.451014</td>\n",
       "      <td>5.246347</td>\n",
       "      <td>5.95035</td>\n",
       "      <td>6.487301</td>\n",
       "      <td>4.937972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1952</th>\n",
       "      <td>8.300677</td>\n",
       "      <td>8.300677</td>\n",
       "      <td>8.300677</td>\n",
       "      <td>8.300677</td>\n",
       "      <td>8.300677</td>\n",
       "      <td>8.300677</td>\n",
       "      <td>8.300677</td>\n",
       "      <td>8.300677</td>\n",
       "      <td>8.300677</td>\n",
       "      <td>8.300677</td>\n",
       "      <td>...</td>\n",
       "      <td>8.528676</td>\n",
       "      <td>9.234682</td>\n",
       "      <td>9.053006</td>\n",
       "      <td>7.765687</td>\n",
       "      <td>5.885682</td>\n",
       "      <td>4.100964</td>\n",
       "      <td>2.876347</td>\n",
       "      <td>2.64035</td>\n",
       "      <td>3.047301</td>\n",
       "      <td>2.927962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1953</th>\n",
       "      <td>10.240677</td>\n",
       "      <td>10.240677</td>\n",
       "      <td>10.240677</td>\n",
       "      <td>10.240677</td>\n",
       "      <td>10.240677</td>\n",
       "      <td>10.240677</td>\n",
       "      <td>10.240677</td>\n",
       "      <td>10.240677</td>\n",
       "      <td>10.240677</td>\n",
       "      <td>10.240677</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.301354</td>\n",
       "      <td>-1.155358</td>\n",
       "      <td>-1.876994</td>\n",
       "      <td>-3.864313</td>\n",
       "      <td>-5.174318</td>\n",
       "      <td>-4.628986</td>\n",
       "      <td>-2.843693</td>\n",
       "      <td>-2.32968</td>\n",
       "      <td>-3.162649</td>\n",
       "      <td>-2.762038</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 324 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Latitude         -90                                                         \\\n",
       "Longitude          0         20         40         60         80        100   \n",
       "1949       -3.649323  -3.649323  -3.649323  -3.649323  -3.649323  -3.649323   \n",
       "1950        1.230677   1.230677   1.230677   1.230677   1.230677   1.230677   \n",
       "1951        7.090677   7.090677   7.090677   7.090677   7.090677   7.090677   \n",
       "1952        8.300677   8.300677   8.300677   8.300677   8.300677   8.300677   \n",
       "1953       10.240677  10.240677  10.240677  10.240677  10.240677  10.240677   \n",
       "\n",
       "Latitude                                               ...        80  \\\n",
       "Longitude        120        140        160        180  ...       160   \n",
       "1949       -3.649323  -3.649323  -3.649323  -3.649323  ...  6.598676   \n",
       "1950        1.230677   1.230677   1.230677   1.230677  ... -4.251374   \n",
       "1951        7.090677   7.090677   7.090677   7.090677  ...  0.478676   \n",
       "1952        8.300677   8.300677   8.300677   8.300677  ...  8.528676   \n",
       "1953       10.240677  10.240677  10.240677  10.240677  ... -1.301354   \n",
       "\n",
       "Latitude                                                               \\\n",
       "Longitude       180       200       220       240       260       280   \n",
       "1949       5.294682  3.563006  1.995687  1.005682  0.951014  1.386347   \n",
       "1950      -4.835348 -5.186994 -4.854313 -2.824318 -0.829026 -1.153683   \n",
       "1951       3.114682  5.042976  5.235657  5.285642  5.451014  5.246347   \n",
       "1952       9.234682  9.053006  7.765687  5.885682  4.100964  2.876347   \n",
       "1953      -1.155358 -1.876994 -3.864313 -5.174318 -4.628986 -2.843693   \n",
       "\n",
       "Latitude                                \n",
       "Longitude      300       320       340  \n",
       "1949       1.43035  1.857351  3.877972  \n",
       "1950      -1.92965 -1.202699  0.617962  \n",
       "1951       5.95035  6.487301  4.937972  \n",
       "1952       2.64035  3.047301  2.927962  \n",
       "1953      -2.32968 -3.162649 -2.762038  \n",
       "\n",
       "[5 rows x 324 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_feature(data):\n",
    "  new_data = []\n",
    "  for i in range(len(data)):\n",
    "    d = data.iloc[i]\n",
    "    new_data.append(list(d))\n",
    "  return np.array(new_data)\n",
    "\n",
    "# Stacking Data of All Months\n",
    "combine_slp, combine_sst, combine_uwnd = [], [], []\n",
    "for i in range(12):\n",
    "  month_path_slp, month_path_sst, month_path_uwnd = slp_months[i], sst_months[i], uwnd_months[i] \n",
    "  month_data_slp ,month_data_sst, month_data_uwnd = pd.read_csv(month_path_slp, header=[0, 1], index_col=0), pd.read_csv(month_path_sst, header=[0, 1], index_col=0), pd.read_csv(month_path_uwnd, header=[0, 1], index_col=0)\n",
    " \n",
    "  feature_slp, feature_sst, feature_uwnd = get_feature(month_data_slp), get_feature(month_data_sst), get_feature(month_data_uwnd)\n",
    "  feature_slp, feature_sst, feature_uwnd = feature_slp[s:e], feature_sst[s:e], feature_uwnd[s:e]\n",
    "  for i in feature_slp:\n",
    "    combine_slp.append(i)\n",
    "  for i in feature_sst:\n",
    "    combine_sst.append(i)\n",
    "  for i in feature_uwnd:\n",
    "    combine_uwnd.append(i)\n",
    "\n",
    "combine = [np.array(combine_slp), np.array(combine_sst), np.array(combine_uwnd)]\n",
    "month_data_slp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Concanating the Data as required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(List):\n",
    "    N = len(List)\n",
    "    if N == 1:\n",
    "        return combine[List[0]]\n",
    "    elif N == 2:\n",
    "        return np.concatenate((combine[List[0]], combine[List[1]]), axis=1)\n",
    "    else:\n",
    "        return np.concatenate((combine[List[0]], combine[List[1]], combine[List[2]]), axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-Encoders (SLP or UWND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 324 - 97 - 323"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_324(n_inputs=324):\n",
    "    visible = Input(shape=(n_inputs,))\n",
    "    e = Dense(n_inputs)(visible)\n",
    "    e = ReLU()(e)\n",
    "    bottleneck = Dense(97)(e)\n",
    "    output = Dense(n_inputs, activation='linear')(bottleneck)\n",
    "    model = Model(inputs=visible, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 97 - 29 - 97"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_97(n_inputs=97):\n",
    "    visible = Input(shape=(n_inputs,))\n",
    "    e = Dense(n_inputs)(visible)\n",
    "    e = ReLU()(e)\n",
    "    bottleneck = Dense(29)(e)\n",
    "    output = Dense(n_inputs, activation='linear')(bottleneck)\n",
    "    model = Model(inputs=visible, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 29 - 9 - 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_29(n_inputs=29):\n",
    "    visible = Input(shape=(n_inputs,))\n",
    "    e = Dense(n_inputs)(visible)\n",
    "    e = ReLU()(e)\n",
    "    bottleneck = Dense(9)(e)\n",
    "    output = Dense(n_inputs, activation='linear')(bottleneck)\n",
    "    model = Model(inputs=visible, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Auto Encoder\n",
    "- 324 - 97 - 29 - 9 - 29 - 97 - 324"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_final_324(n_inputs=324):\n",
    "    visible = Input(shape=(n_inputs,))\n",
    "    e = Dense(n_inputs)(visible)\n",
    "    e = ReLU()(e)\n",
    "    e = Dense(97)(e)\n",
    "    e = ReLU()(e)\n",
    "    e = Dense(29)(e)\n",
    "    e = ReLU()(e)\n",
    "    bottleneck = Dense(9)(e)\n",
    "    e = Dense(29)(bottleneck)\n",
    "    e = ReLU()(e)\n",
    "    e = Dense(97)(e)\n",
    "    e = ReLU()(e)\n",
    "    output = Dense(n_inputs, activation='linear')(e)\n",
    "    model = Model(inputs=visible, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-Encoders (SST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 192 - 72 - 192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_192(n_inputs=192):\n",
    "    visible = Input(shape=(n_inputs,))\n",
    "    e = Dense(n_inputs)(visible)\n",
    "    e = ReLU()(e)\n",
    "    bottleneck = Dense(72)(e)\n",
    "    output = Dense(n_inputs, activation='linear')(bottleneck)\n",
    "    model = Model(inputs=visible, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 72 - 21 - 72"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_72(n_inputs=72):\n",
    "    visible = Input(shape=(n_inputs,))\n",
    "    e = Dense(n_inputs)(visible)\n",
    "    e = ReLU()(e)\n",
    "    bottleneck = Dense(21)(e)\n",
    "    output = Dense(n_inputs, activation='linear')(bottleneck)\n",
    "    model = Model(inputs=visible, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 21 - 6 - 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_21(n_inputs=21):\n",
    "    visible = Input(shape=(n_inputs,))\n",
    "    e = Dense(n_inputs)(visible)\n",
    "    e = ReLU()(e)\n",
    "    bottleneck = Dense(6)(e)\n",
    "    output = Dense(n_inputs, activation='linear')(bottleneck)\n",
    "    model = Model(inputs=visible, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Auto Encoder\n",
    "- 192 - 72 - 21 - 6 - 21 - 72 - 192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_final_192(n_inputs=192):\n",
    "    visible = Input(shape=(n_inputs,))\n",
    "    e = Dense(n_inputs)(visible)\n",
    "    e = ReLU()(e)\n",
    "    e = Dense(72)(e)\n",
    "    e = ReLU()(e)\n",
    "    e = Dense(21)(e)\n",
    "    e = ReLU()(e)\n",
    "    bottleneck = Dense(6)(e)\n",
    "    e = Dense(21)(bottleneck)\n",
    "    e = ReLU()(e)\n",
    "    e = Dense(72)(e)\n",
    "    e = ReLU()(e)\n",
    "    output = Dense(n_inputs, activation='linear')(e)\n",
    "    model = Model(inputs=visible, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-Encoders (SST-SLP or SST-UWND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 416 - 169 - 416"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_416(n_inputs=416):\n",
    "    visible = Input(shape=(n_inputs,))\n",
    "    e = Dense(n_inputs)(visible)\n",
    "    e = ReLU()(e)\n",
    "    bottleneck = Dense(169)(e)\n",
    "    output = Dense(n_inputs, activation='linear')(bottleneck)\n",
    "    model = Model(inputs=visible, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 169 - 50 - 169"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_169(n_inputs=169):\n",
    "    visible = Input(shape=(n_inputs,))\n",
    "    e = Dense(n_inputs)(visible)\n",
    "    e = ReLU()(e)\n",
    "    bottleneck = Dense(50)(e)\n",
    "    output = Dense(n_inputs, activation='linear')(bottleneck)\n",
    "    model = Model(inputs=visible, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 50 - 15 - 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_50(n_inputs=50):\n",
    "    visible = Input(shape=(n_inputs,))\n",
    "    e = Dense(n_inputs)(visible)\n",
    "    e = ReLU()(e)\n",
    "    bottleneck = Dense(15)(e)\n",
    "    output = Dense(n_inputs, activation='linear')(bottleneck)\n",
    "    model = Model(inputs=visible, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Auto Encoder\n",
    "- 416 - 169 - 50 - 15 - 50 - 169 - 416"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_final_416(n_inputs=416):\n",
    "    visible = Input(shape=(n_inputs,))\n",
    "    e = Dense(n_inputs)(visible)\n",
    "    e = ReLU()(e)\n",
    "    e = Dense(169)(e)\n",
    "    e = ReLU()(e)\n",
    "    e = Dense(21)(e)\n",
    "    e = ReLU()(e)\n",
    "    bottleneck = Dense(6)(e)\n",
    "    e = Dense(21)(bottleneck)\n",
    "    e = ReLU()(e)\n",
    "    e = Dense(169)(e)\n",
    "    e = ReLU()(e)\n",
    "    output = Dense(n_inputs, activation='linear')(e)\n",
    "    model = Model(inputs=visible, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto-Encoders (SLP-UWND)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 648 - 194 - 648"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_648(n_inputs=648):\n",
    "    visible = Input(shape=(n_inputs,))\n",
    "    e = Dense(n_inputs)(visible)\n",
    "    e = ReLU()(e)\n",
    "    bottleneck = Dense(194)(e)\n",
    "    output = Dense(n_inputs, activation='linear')(bottleneck)\n",
    "    model = Model(inputs=visible, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 194 - 58 - 194"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_194(n_inputs=194):\n",
    "    visible = Input(shape=(n_inputs,))\n",
    "    e = Dense(n_inputs)(visible)\n",
    "    e = ReLU()(e)\n",
    "    bottleneck = Dense(58)(e)\n",
    "    output = Dense(n_inputs, activation='linear')(bottleneck)\n",
    "    model = Model(inputs=visible, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 58 - 17 - 58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_58(n_inputs=58):\n",
    "    visible = Input(shape=(n_inputs,))\n",
    "    e = Dense(n_inputs)(visible)\n",
    "    e = ReLU()(e)\n",
    "    bottleneck = Dense(17)(e)\n",
    "    output = Dense(n_inputs, activation='linear')(bottleneck)\n",
    "    model = Model(inputs=visible, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Auto Encoder\n",
    "- 648 - 194 - 58 - 17 - 58 - 194 - 648"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_final_648(n_inputs=648):\n",
    "    visible = Input(shape=(n_inputs,))\n",
    "    e = Dense(n_inputs)(visible)\n",
    "    e = ReLU()(e)\n",
    "    e = Dense(194)(e)\n",
    "    e = ReLU()(e)\n",
    "    e = Dense(58)(e)\n",
    "    e = ReLU()(e)\n",
    "    bottleneck = Dense(17)(e)\n",
    "    e = Dense(58)(bottleneck)\n",
    "    e = ReLU()(e)\n",
    "    e = Dense(194)(e)\n",
    "    e = ReLU()(e)\n",
    "    output = Dense(n_inputs, activation='linear')(e)\n",
    "    model = Model(inputs=visible, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get all Models in a 2-D List\n",
    "|       Outer Most Layer           |        Middle Layer             |      Inner Most Layer         |      Full Model                              |\n",
    "|----------------------------------|---------------------------------|-------------------------------|----------------------------------------------|\n",
    "| [ Model SLP  - 324-97-324        |  Model SLP  - 97-29-97          |  Model SLP  - 29-9-29         |  Model SLP  - 324-97-29-9-29-97-324  ]       |\n",
    "| [ Model UWND - 324-97-324        |   Model UWND - 97-29-97         |   Model UWND - 29-9-29        |   Model UWND - 324-97-29-9-29-97-324 ]       |\n",
    "| [ Model SST  - 192-72-192        |   Model SST  - 72-21-72         |   Model SST  - 21-6-21        |   Model SST  - 192-72-21-6-21-72-192 ]       |\n",
    "| [ Model SLP-SST  - 416-169-416   |   Model SLP-SST  - 169-50-169   |   Model SLP-SST  - 50-15-50   |   Model SLP-SST  - 416-169-50-15-50-169-416] |\n",
    "| [ Model UWND-SST - 416-169-416   |   Model UWND-SST - 169-50-169   |   Model UWND-SST - 50-15-50   |   Model UWND-SST - 416-169-50-15-50-169-416] |\n",
    "| [ Model SLP-UWND - 648-194-648   |   Model SLP-UWND - 194-58-194   |   Model SLP-UWND - 58-17-58   |   Model SLP-UWND - 416-194-58-17-58-194-416] |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_model():\n",
    "    models_slp  = [get_model_324(), get_model_97(), get_model_29(), get_model_final_324()]\n",
    "    models_uwnd = [get_model_324(), get_model_97(), get_model_29(), get_model_final_324()]\n",
    "    models_sst  = [get_model_192(), get_model_72(), get_model_21(), get_model_final_192()]\n",
    "    models_slp_sst  = [get_model_416(), get_model_169(), get_model_50(), get_model_final_416()]\n",
    "    models_uwnd_sst = [get_model_416(), get_model_169(), get_model_50(), get_model_final_416()]\n",
    "    models_slp_uwnd = [get_model_648(), get_model_194(), get_model_58(), get_model_final_648()]\n",
    "\n",
    "    models = [models_slp, models_uwnd, models_sst, models_slp_sst, models_uwnd_sst, models_slp_uwnd]\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = get_all_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 324)]             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 324)               105300    \n",
      "                                                                 \n",
      " re_lu (ReLU)                (None, 324)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 97)                31525     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 324)               31752     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 168,577\n",
      "Trainable params: 168,577\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 97)]              0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 97)                9506      \n",
      "                                                                 \n",
      " re_lu_1 (ReLU)              (None, 97)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 29)                2842      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 97)                2910      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 15,258\n",
      "Trainable params: 15,258\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 29)]              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 29)                870       \n",
      "                                                                 \n",
      " re_lu_2 (ReLU)              (None, 29)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 9)                 270       \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 29)                290       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,430\n",
      "Trainable params: 1,430\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, 324)]             0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 324)               105300    \n",
      "                                                                 \n",
      " re_lu_3 (ReLU)              (None, 324)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 97)                31525     \n",
      "                                                                 \n",
      " re_lu_4 (ReLU)              (None, 97)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 29)                2842      \n",
      "                                                                 \n",
      " re_lu_5 (ReLU)              (None, 29)                0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 9)                 270       \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 29)                290       \n",
      "                                                                 \n",
      " re_lu_6 (ReLU)              (None, 29)                0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 97)                2910      \n",
      "                                                                 \n",
      " re_lu_7 (ReLU)              (None, 97)                0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 324)               31752     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 174,889\n",
      "Trainable params: 174,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for i in models[0]:\n",
    "    i.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1248, 324)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models = get_all_model()\n",
    "data = np.concatenate((get_data([0]), get_data([2])), axis=0)\n",
    "# data = get_data([0])\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_next_inputs(models, index, inputs):\n",
    "    model_temp = Model(inputs=models[index].input, outputs=models[index].layers[3].output)\n",
    "    return model_temp(inputs)\n",
    "\n",
    "def train_all_models(models, inputs):\n",
    "    x = inputs\n",
    "    if type(models) == type([]):\n",
    "        for i, model in enumerate(models[:-1]):\n",
    "            model.fit(x, x, epochs=800, batch_size=16, verbose=2, validation_data=(x,x))\n",
    "            x = get_next_inputs(models, i, x)\n",
    "\n",
    "        models[-1].layers[2] = models[0].layers[3]\n",
    "        models[-1].layers[11]= models[0].layers[4]\n",
    "        models[-1].layers[4] = models[1].layers[3]\n",
    "        models[-1].layers[9] = models[1].layers[4]\n",
    "        models[-1].layers[6] = models[2].layers[3]\n",
    "        models[-1].layers[7] = models[2].layers[4]\n",
    "        models[-1].compile(optimizer='adam', loss='mse')\n",
    "\n",
    "        model[-1].fit(inputs, inputs, epochs=800, batch_size=16, verbose=2, validation_data=(inputs,inputs))\n",
    "\n",
    "def train_perticular_model(models, index, inputs):\n",
    "    if not index == 0 and not index == -1:\n",
    "        x = get_next_inputs(models, index-1, inputs)\n",
    "    else:\n",
    "        x = inputs\n",
    "    models[index].fit(x, x, epochs=800, batch_size=16, verbose=2, validation_data=(x,x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/800\n",
      "78/78 - 0s - loss: 17.6789 - val_loss: 15.3070 - 467ms/epoch - 6ms/step\n",
      "Epoch 2/800\n",
      "78/78 - 0s - loss: 14.5371 - val_loss: 13.8178 - 118ms/epoch - 2ms/step\n",
      "Epoch 3/800\n",
      "78/78 - 0s - loss: 13.6829 - val_loss: 13.2415 - 117ms/epoch - 1ms/step\n",
      "Epoch 4/800\n",
      "78/78 - 0s - loss: 13.2030 - val_loss: 12.8798 - 117ms/epoch - 1ms/step\n",
      "Epoch 5/800\n",
      "78/78 - 0s - loss: 12.8430 - val_loss: 12.5198 - 121ms/epoch - 2ms/step\n",
      "Epoch 6/800\n",
      "78/78 - 0s - loss: 12.4618 - val_loss: 12.1043 - 132ms/epoch - 2ms/step\n",
      "Epoch 7/800\n",
      "78/78 - 0s - loss: 12.1253 - val_loss: 11.8168 - 125ms/epoch - 2ms/step\n",
      "Epoch 8/800\n",
      "78/78 - 0s - loss: 11.8481 - val_loss: 11.5353 - 121ms/epoch - 2ms/step\n",
      "Epoch 9/800\n",
      "78/78 - 0s - loss: 11.6131 - val_loss: 11.3501 - 122ms/epoch - 2ms/step\n",
      "Epoch 10/800\n",
      "78/78 - 0s - loss: 11.3887 - val_loss: 11.1361 - 119ms/epoch - 2ms/step\n",
      "Epoch 11/800\n",
      "78/78 - 0s - loss: 11.2086 - val_loss: 11.0055 - 122ms/epoch - 2ms/step\n",
      "Epoch 12/800\n",
      "78/78 - 0s - loss: 11.0989 - val_loss: 10.8884 - 121ms/epoch - 2ms/step\n",
      "Epoch 13/800\n",
      "78/78 - 0s - loss: 10.9751 - val_loss: 10.7371 - 119ms/epoch - 2ms/step\n",
      "Epoch 14/800\n",
      "78/78 - 0s - loss: 10.8559 - val_loss: 10.6425 - 120ms/epoch - 2ms/step\n",
      "Epoch 15/800\n",
      "78/78 - 0s - loss: 10.7572 - val_loss: 10.6347 - 120ms/epoch - 2ms/step\n",
      "Epoch 16/800\n",
      "78/78 - 0s - loss: 10.6620 - val_loss: 10.4146 - 120ms/epoch - 2ms/step\n",
      "Epoch 17/800\n",
      "78/78 - 0s - loss: 10.5505 - val_loss: 10.3244 - 117ms/epoch - 1ms/step\n",
      "Epoch 18/800\n",
      "78/78 - 0s - loss: 10.4506 - val_loss: 10.2559 - 121ms/epoch - 2ms/step\n",
      "Epoch 19/800\n",
      "78/78 - 0s - loss: 10.3719 - val_loss: 10.1782 - 119ms/epoch - 2ms/step\n",
      "Epoch 20/800\n",
      "78/78 - 0s - loss: 10.2606 - val_loss: 10.1219 - 119ms/epoch - 2ms/step\n",
      "Epoch 21/800\n",
      "78/78 - 0s - loss: 10.1773 - val_loss: 10.0010 - 118ms/epoch - 2ms/step\n",
      "Epoch 22/800\n",
      "78/78 - 0s - loss: 10.1151 - val_loss: 9.9634 - 120ms/epoch - 2ms/step\n",
      "Epoch 23/800\n",
      "78/78 - 0s - loss: 10.0707 - val_loss: 9.8619 - 121ms/epoch - 2ms/step\n",
      "Epoch 24/800\n",
      "78/78 - 0s - loss: 9.9862 - val_loss: 9.7866 - 118ms/epoch - 2ms/step\n",
      "Epoch 25/800\n",
      "78/78 - 0s - loss: 9.9060 - val_loss: 9.7319 - 121ms/epoch - 2ms/step\n",
      "Epoch 26/800\n",
      "78/78 - 0s - loss: 9.8446 - val_loss: 9.6497 - 119ms/epoch - 2ms/step\n",
      "Epoch 27/800\n",
      "78/78 - 0s - loss: 9.7671 - val_loss: 9.5480 - 123ms/epoch - 2ms/step\n",
      "Epoch 28/800\n",
      "78/78 - 0s - loss: 9.7188 - val_loss: 9.5563 - 119ms/epoch - 2ms/step\n",
      "Epoch 29/800\n",
      "78/78 - 0s - loss: 9.6581 - val_loss: 9.4773 - 121ms/epoch - 2ms/step\n",
      "Epoch 30/800\n",
      "78/78 - 0s - loss: 9.6252 - val_loss: 9.4103 - 122ms/epoch - 2ms/step\n",
      "Epoch 31/800\n",
      "78/78 - 0s - loss: 9.5659 - val_loss: 9.3797 - 120ms/epoch - 2ms/step\n",
      "Epoch 32/800\n",
      "78/78 - 0s - loss: 9.4982 - val_loss: 9.2876 - 126ms/epoch - 2ms/step\n",
      "Epoch 33/800\n",
      "78/78 - 0s - loss: 9.4438 - val_loss: 9.2177 - 123ms/epoch - 2ms/step\n",
      "Epoch 34/800\n",
      "78/78 - 0s - loss: 9.3848 - val_loss: 9.2230 - 119ms/epoch - 2ms/step\n",
      "Epoch 35/800\n",
      "78/78 - 0s - loss: 9.3838 - val_loss: 9.1847 - 119ms/epoch - 2ms/step\n",
      "Epoch 36/800\n",
      "78/78 - 0s - loss: 9.3212 - val_loss: 9.1030 - 119ms/epoch - 2ms/step\n",
      "Epoch 37/800\n",
      "78/78 - 0s - loss: 9.2488 - val_loss: 9.0507 - 121ms/epoch - 2ms/step\n",
      "Epoch 38/800\n",
      "78/78 - 0s - loss: 9.2030 - val_loss: 8.9980 - 121ms/epoch - 2ms/step\n",
      "Epoch 39/800\n",
      "78/78 - 0s - loss: 9.1521 - val_loss: 8.9654 - 118ms/epoch - 2ms/step\n",
      "Epoch 40/800\n",
      "78/78 - 0s - loss: 9.1453 - val_loss: 8.9362 - 123ms/epoch - 2ms/step\n",
      "Epoch 41/800\n",
      "78/78 - 0s - loss: 9.1312 - val_loss: 8.9331 - 120ms/epoch - 2ms/step\n",
      "Epoch 42/800\n",
      "78/78 - 0s - loss: 9.0883 - val_loss: 8.8851 - 122ms/epoch - 2ms/step\n",
      "Epoch 43/800\n",
      "78/78 - 0s - loss: 9.0252 - val_loss: 8.8512 - 121ms/epoch - 2ms/step\n",
      "Epoch 44/800\n",
      "78/78 - 0s - loss: 9.0307 - val_loss: 8.8324 - 118ms/epoch - 2ms/step\n",
      "Epoch 45/800\n",
      "78/78 - 0s - loss: 8.9681 - val_loss: 8.7302 - 120ms/epoch - 2ms/step\n",
      "Epoch 46/800\n",
      "78/78 - 0s - loss: 8.9101 - val_loss: 8.7098 - 120ms/epoch - 2ms/step\n",
      "Epoch 47/800\n",
      "78/78 - 0s - loss: 8.9037 - val_loss: 8.7444 - 120ms/epoch - 2ms/step\n",
      "Epoch 48/800\n",
      "78/78 - 0s - loss: 8.8906 - val_loss: 8.6755 - 121ms/epoch - 2ms/step\n",
      "Epoch 49/800\n",
      "78/78 - 0s - loss: 8.8557 - val_loss: 8.6347 - 122ms/epoch - 2ms/step\n",
      "Epoch 50/800\n",
      "78/78 - 0s - loss: 8.7996 - val_loss: 8.5927 - 118ms/epoch - 2ms/step\n",
      "Epoch 51/800\n",
      "78/78 - 0s - loss: 8.7567 - val_loss: 8.6122 - 120ms/epoch - 2ms/step\n",
      "Epoch 52/800\n",
      "78/78 - 0s - loss: 8.7370 - val_loss: 8.5433 - 119ms/epoch - 2ms/step\n",
      "Epoch 53/800\n",
      "78/78 - 0s - loss: 8.7310 - val_loss: 8.5707 - 123ms/epoch - 2ms/step\n",
      "Epoch 54/800\n",
      "78/78 - 0s - loss: 8.6794 - val_loss: 8.4965 - 149ms/epoch - 2ms/step\n",
      "Epoch 55/800\n",
      "78/78 - 0s - loss: 8.6895 - val_loss: 8.5271 - 119ms/epoch - 2ms/step\n",
      "Epoch 56/800\n",
      "78/78 - 0s - loss: 8.6508 - val_loss: 8.4311 - 119ms/epoch - 2ms/step\n",
      "Epoch 57/800\n",
      "78/78 - 0s - loss: 8.5850 - val_loss: 8.3962 - 122ms/epoch - 2ms/step\n",
      "Epoch 58/800\n",
      "78/78 - 0s - loss: 8.5657 - val_loss: 8.3732 - 119ms/epoch - 2ms/step\n",
      "Epoch 59/800\n",
      "78/78 - 0s - loss: 8.5677 - val_loss: 8.3508 - 119ms/epoch - 2ms/step\n",
      "Epoch 60/800\n",
      "78/78 - 0s - loss: 8.5195 - val_loss: 8.3405 - 118ms/epoch - 2ms/step\n",
      "Epoch 61/800\n",
      "78/78 - 0s - loss: 8.5226 - val_loss: 8.3348 - 122ms/epoch - 2ms/step\n",
      "Epoch 62/800\n",
      "78/78 - 0s - loss: 8.5047 - val_loss: 8.3297 - 123ms/epoch - 2ms/step\n",
      "Epoch 63/800\n",
      "78/78 - 0s - loss: 8.5208 - val_loss: 8.3119 - 118ms/epoch - 2ms/step\n",
      "Epoch 64/800\n",
      "78/78 - 0s - loss: 8.4626 - val_loss: 8.2256 - 118ms/epoch - 2ms/step\n",
      "Epoch 65/800\n",
      "78/78 - 0s - loss: 8.4301 - val_loss: 8.2508 - 118ms/epoch - 2ms/step\n",
      "Epoch 66/800\n",
      "78/78 - 0s - loss: 8.4184 - val_loss: 8.2310 - 122ms/epoch - 2ms/step\n",
      "Epoch 67/800\n",
      "78/78 - 0s - loss: 8.3891 - val_loss: 8.2086 - 121ms/epoch - 2ms/step\n",
      "Epoch 68/800\n",
      "78/78 - 0s - loss: 8.3809 - val_loss: 8.1764 - 117ms/epoch - 1ms/step\n",
      "Epoch 69/800\n",
      "78/78 - 0s - loss: 8.3586 - val_loss: 8.1706 - 115ms/epoch - 1ms/step\n",
      "Epoch 70/800\n",
      "78/78 - 0s - loss: 8.3347 - val_loss: 8.0887 - 121ms/epoch - 2ms/step\n",
      "Epoch 71/800\n",
      "78/78 - 0s - loss: 8.3115 - val_loss: 8.1960 - 123ms/epoch - 2ms/step\n",
      "Epoch 72/800\n",
      "78/78 - 0s - loss: 8.2958 - val_loss: 8.0986 - 120ms/epoch - 2ms/step\n",
      "Epoch 73/800\n",
      "78/78 - 0s - loss: 8.2678 - val_loss: 8.0609 - 121ms/epoch - 2ms/step\n",
      "Epoch 74/800\n",
      "78/78 - 0s - loss: 8.2416 - val_loss: 8.1027 - 117ms/epoch - 2ms/step\n",
      "Epoch 75/800\n",
      "78/78 - 0s - loss: 8.2513 - val_loss: 8.1214 - 120ms/epoch - 2ms/step\n",
      "Epoch 76/800\n",
      "78/78 - 0s - loss: 8.2571 - val_loss: 8.0926 - 121ms/epoch - 2ms/step\n",
      "Epoch 77/800\n",
      "78/78 - 0s - loss: 8.2392 - val_loss: 8.0651 - 122ms/epoch - 2ms/step\n",
      "Epoch 78/800\n",
      "78/78 - 0s - loss: 8.2202 - val_loss: 8.0089 - 122ms/epoch - 2ms/step\n",
      "Epoch 79/800\n",
      "78/78 - 0s - loss: 8.2016 - val_loss: 7.9929 - 120ms/epoch - 2ms/step\n",
      "Epoch 80/800\n",
      "78/78 - 0s - loss: 8.1573 - val_loss: 7.9449 - 122ms/epoch - 2ms/step\n",
      "Epoch 81/800\n",
      "78/78 - 0s - loss: 8.1631 - val_loss: 7.9878 - 121ms/epoch - 2ms/step\n",
      "Epoch 82/800\n",
      "78/78 - 0s - loss: 8.1428 - val_loss: 7.9936 - 120ms/epoch - 2ms/step\n",
      "Epoch 83/800\n",
      "78/78 - 0s - loss: 8.1496 - val_loss: 7.9949 - 122ms/epoch - 2ms/step\n",
      "Epoch 84/800\n",
      "78/78 - 0s - loss: 8.1099 - val_loss: 7.9294 - 119ms/epoch - 2ms/step\n",
      "Epoch 85/800\n",
      "78/78 - 0s - loss: 8.1017 - val_loss: 7.9491 - 119ms/epoch - 2ms/step\n",
      "Epoch 86/800\n",
      "78/78 - 0s - loss: 8.0823 - val_loss: 7.8703 - 121ms/epoch - 2ms/step\n",
      "Epoch 87/800\n",
      "78/78 - 0s - loss: 8.0418 - val_loss: 7.8599 - 120ms/epoch - 2ms/step\n",
      "Epoch 88/800\n",
      "78/78 - 0s - loss: 8.0537 - val_loss: 7.9036 - 118ms/epoch - 2ms/step\n",
      "Epoch 89/800\n",
      "78/78 - 0s - loss: 8.0686 - val_loss: 7.9226 - 126ms/epoch - 2ms/step\n",
      "Epoch 90/800\n",
      "78/78 - 0s - loss: 8.0535 - val_loss: 7.8590 - 117ms/epoch - 1ms/step\n",
      "Epoch 91/800\n",
      "78/78 - 0s - loss: 8.0336 - val_loss: 7.8474 - 122ms/epoch - 2ms/step\n",
      "Epoch 92/800\n",
      "78/78 - 0s - loss: 7.9910 - val_loss: 7.7760 - 117ms/epoch - 1ms/step\n",
      "Epoch 93/800\n",
      "78/78 - 0s - loss: 7.9802 - val_loss: 7.7914 - 122ms/epoch - 2ms/step\n",
      "Epoch 94/800\n",
      "78/78 - 0s - loss: 7.9790 - val_loss: 7.7899 - 121ms/epoch - 2ms/step\n",
      "Epoch 95/800\n",
      "78/78 - 0s - loss: 7.9537 - val_loss: 7.7882 - 121ms/epoch - 2ms/step\n",
      "Epoch 96/800\n",
      "78/78 - 0s - loss: 7.9408 - val_loss: 7.7653 - 154ms/epoch - 2ms/step\n",
      "Epoch 97/800\n",
      "78/78 - 0s - loss: 7.9593 - val_loss: 7.8243 - 119ms/epoch - 2ms/step\n",
      "Epoch 98/800\n",
      "78/78 - 0s - loss: 7.9719 - val_loss: 7.7692 - 119ms/epoch - 2ms/step\n",
      "Epoch 99/800\n",
      "78/78 - 0s - loss: 7.9580 - val_loss: 7.7476 - 119ms/epoch - 2ms/step\n",
      "Epoch 100/800\n",
      "78/78 - 0s - loss: 7.9222 - val_loss: 7.7214 - 119ms/epoch - 2ms/step\n",
      "Epoch 101/800\n",
      "78/78 - 0s - loss: 7.9045 - val_loss: 7.7056 - 118ms/epoch - 2ms/step\n",
      "Epoch 102/800\n",
      "78/78 - 0s - loss: 7.8727 - val_loss: 7.7131 - 117ms/epoch - 1ms/step\n",
      "Epoch 103/800\n",
      "78/78 - 0s - loss: 7.8644 - val_loss: 7.6705 - 119ms/epoch - 2ms/step\n",
      "Epoch 104/800\n",
      "78/78 - 0s - loss: 7.8701 - val_loss: 7.6904 - 122ms/epoch - 2ms/step\n",
      "Epoch 105/800\n",
      "78/78 - 0s - loss: 7.8372 - val_loss: 7.6936 - 122ms/epoch - 2ms/step\n",
      "Epoch 106/800\n",
      "78/78 - 0s - loss: 7.8633 - val_loss: 7.6905 - 119ms/epoch - 2ms/step\n",
      "Epoch 107/800\n",
      "78/78 - 0s - loss: 7.8517 - val_loss: 7.6556 - 119ms/epoch - 2ms/step\n",
      "Epoch 108/800\n",
      "78/78 - 0s - loss: 7.8683 - val_loss: 7.6821 - 121ms/epoch - 2ms/step\n",
      "Epoch 109/800\n",
      "78/78 - 0s - loss: 7.8558 - val_loss: 7.6482 - 120ms/epoch - 2ms/step\n",
      "Epoch 110/800\n",
      "78/78 - 0s - loss: 7.8267 - val_loss: 7.7153 - 118ms/epoch - 2ms/step\n",
      "Epoch 111/800\n",
      "78/78 - 0s - loss: 7.8446 - val_loss: 7.6031 - 118ms/epoch - 2ms/step\n",
      "Epoch 112/800\n",
      "78/78 - 0s - loss: 7.8303 - val_loss: 7.6524 - 117ms/epoch - 2ms/step\n",
      "Epoch 113/800\n",
      "78/78 - 0s - loss: 7.8029 - val_loss: 7.6471 - 122ms/epoch - 2ms/step\n",
      "Epoch 114/800\n",
      "78/78 - 0s - loss: 7.7570 - val_loss: 7.5609 - 120ms/epoch - 2ms/step\n",
      "Epoch 115/800\n",
      "78/78 - 0s - loss: 7.7383 - val_loss: 7.5748 - 123ms/epoch - 2ms/step\n",
      "Epoch 116/800\n",
      "78/78 - 0s - loss: 7.7415 - val_loss: 7.5589 - 122ms/epoch - 2ms/step\n",
      "Epoch 117/800\n",
      "78/78 - 0s - loss: 7.7190 - val_loss: 7.5428 - 124ms/epoch - 2ms/step\n",
      "Epoch 118/800\n",
      "78/78 - 0s - loss: 7.7305 - val_loss: 7.5253 - 125ms/epoch - 2ms/step\n",
      "Epoch 119/800\n",
      "78/78 - 0s - loss: 7.7591 - val_loss: 7.6144 - 125ms/epoch - 2ms/step\n",
      "Epoch 120/800\n",
      "78/78 - 0s - loss: 7.7725 - val_loss: 7.5485 - 124ms/epoch - 2ms/step\n",
      "Epoch 121/800\n",
      "78/78 - 0s - loss: 7.7645 - val_loss: 7.5657 - 126ms/epoch - 2ms/step\n",
      "Epoch 122/800\n",
      "78/78 - 0s - loss: 7.7565 - val_loss: 7.5410 - 123ms/epoch - 2ms/step\n",
      "Epoch 123/800\n",
      "78/78 - 0s - loss: 7.7421 - val_loss: 7.5346 - 126ms/epoch - 2ms/step\n",
      "Epoch 124/800\n",
      "78/78 - 0s - loss: 7.7023 - val_loss: 7.5326 - 122ms/epoch - 2ms/step\n",
      "Epoch 125/800\n",
      "78/78 - 0s - loss: 7.6915 - val_loss: 7.4664 - 119ms/epoch - 2ms/step\n",
      "Epoch 126/800\n",
      "78/78 - 0s - loss: 7.6770 - val_loss: 7.5122 - 121ms/epoch - 2ms/step\n",
      "Epoch 127/800\n",
      "78/78 - 0s - loss: 7.6814 - val_loss: 7.5602 - 120ms/epoch - 2ms/step\n",
      "Epoch 128/800\n",
      "78/78 - 0s - loss: 7.7163 - val_loss: 7.5249 - 123ms/epoch - 2ms/step\n",
      "Epoch 129/800\n",
      "78/78 - 0s - loss: 7.7007 - val_loss: 7.5305 - 124ms/epoch - 2ms/step\n",
      "Epoch 130/800\n",
      "78/78 - 0s - loss: 7.6749 - val_loss: 7.4896 - 126ms/epoch - 2ms/step\n",
      "Epoch 131/800\n",
      "78/78 - 0s - loss: 7.6837 - val_loss: 7.5098 - 123ms/epoch - 2ms/step\n",
      "Epoch 132/800\n",
      "78/78 - 0s - loss: 7.6551 - val_loss: 7.4765 - 121ms/epoch - 2ms/step\n",
      "Epoch 133/800\n",
      "78/78 - 0s - loss: 7.6237 - val_loss: 7.4650 - 126ms/epoch - 2ms/step\n",
      "Epoch 134/800\n",
      "78/78 - 0s - loss: 7.6331 - val_loss: 7.4291 - 120ms/epoch - 2ms/step\n",
      "Epoch 135/800\n",
      "78/78 - 0s - loss: 7.6264 - val_loss: 7.4798 - 127ms/epoch - 2ms/step\n",
      "Epoch 136/800\n",
      "78/78 - 0s - loss: 7.6311 - val_loss: 7.4305 - 122ms/epoch - 2ms/step\n",
      "Epoch 137/800\n",
      "78/78 - 0s - loss: 7.6494 - val_loss: 7.4813 - 119ms/epoch - 2ms/step\n",
      "Epoch 138/800\n",
      "78/78 - 0s - loss: 7.6081 - val_loss: 7.4814 - 123ms/epoch - 2ms/step\n",
      "Epoch 139/800\n",
      "78/78 - 0s - loss: 7.6000 - val_loss: 7.4046 - 122ms/epoch - 2ms/step\n",
      "Epoch 140/800\n",
      "78/78 - 0s - loss: 7.5808 - val_loss: 7.4486 - 119ms/epoch - 2ms/step\n",
      "Epoch 141/800\n",
      "78/78 - 0s - loss: 7.5828 - val_loss: 7.3964 - 119ms/epoch - 2ms/step\n",
      "Epoch 142/800\n",
      "78/78 - 0s - loss: 7.5843 - val_loss: 7.4279 - 121ms/epoch - 2ms/step\n",
      "Epoch 143/800\n",
      "78/78 - 0s - loss: 7.5738 - val_loss: 7.3958 - 120ms/epoch - 2ms/step\n",
      "Epoch 144/800\n",
      "78/78 - 0s - loss: 7.5922 - val_loss: 7.3928 - 118ms/epoch - 2ms/step\n",
      "Epoch 145/800\n",
      "78/78 - 0s - loss: 7.5511 - val_loss: 7.4218 - 118ms/epoch - 2ms/step\n",
      "Epoch 146/800\n",
      "78/78 - 0s - loss: 7.5645 - val_loss: 7.3572 - 124ms/epoch - 2ms/step\n",
      "Epoch 147/800\n",
      "78/78 - 0s - loss: 7.5613 - val_loss: 7.3520 - 120ms/epoch - 2ms/step\n",
      "Epoch 148/800\n",
      "78/78 - 0s - loss: 7.5396 - val_loss: 7.3540 - 120ms/epoch - 2ms/step\n",
      "Epoch 149/800\n",
      "78/78 - 0s - loss: 7.5454 - val_loss: 7.3959 - 122ms/epoch - 2ms/step\n",
      "Epoch 150/800\n",
      "78/78 - 0s - loss: 7.5535 - val_loss: 7.3539 - 119ms/epoch - 2ms/step\n",
      "Epoch 151/800\n",
      "78/78 - 0s - loss: 7.5338 - val_loss: 7.3244 - 127ms/epoch - 2ms/step\n",
      "Epoch 152/800\n",
      "78/78 - 0s - loss: 7.4945 - val_loss: 7.3261 - 124ms/epoch - 2ms/step\n",
      "Epoch 153/800\n",
      "78/78 - 0s - loss: 7.5259 - val_loss: 7.3314 - 120ms/epoch - 2ms/step\n",
      "Epoch 154/800\n",
      "78/78 - 0s - loss: 7.5215 - val_loss: 7.3632 - 122ms/epoch - 2ms/step\n",
      "Epoch 155/800\n",
      "78/78 - 0s - loss: 7.5486 - val_loss: 7.3567 - 119ms/epoch - 2ms/step\n",
      "Epoch 156/800\n",
      "78/78 - 0s - loss: 7.5253 - val_loss: 7.3609 - 121ms/epoch - 2ms/step\n",
      "Epoch 157/800\n",
      "78/78 - 0s - loss: 7.5146 - val_loss: 7.3170 - 123ms/epoch - 2ms/step\n",
      "Epoch 158/800\n",
      "78/78 - 0s - loss: 7.4787 - val_loss: 7.3105 - 130ms/epoch - 2ms/step\n",
      "Epoch 159/800\n",
      "78/78 - 0s - loss: 7.4759 - val_loss: 7.2852 - 127ms/epoch - 2ms/step\n",
      "Epoch 160/800\n",
      "78/78 - 0s - loss: 7.4639 - val_loss: 7.3196 - 123ms/epoch - 2ms/step\n",
      "Epoch 161/800\n",
      "78/78 - 0s - loss: 7.5068 - val_loss: 7.3132 - 149ms/epoch - 2ms/step\n",
      "Epoch 162/800\n",
      "78/78 - 0s - loss: 7.5043 - val_loss: 7.3020 - 126ms/epoch - 2ms/step\n",
      "Epoch 163/800\n",
      "78/78 - 0s - loss: 7.4958 - val_loss: 7.2992 - 122ms/epoch - 2ms/step\n",
      "Epoch 164/800\n",
      "78/78 - 0s - loss: 7.4415 - val_loss: 7.2431 - 123ms/epoch - 2ms/step\n",
      "Epoch 165/800\n",
      "78/78 - 0s - loss: 7.4263 - val_loss: 7.2667 - 122ms/epoch - 2ms/step\n",
      "Epoch 166/800\n",
      "78/78 - 0s - loss: 7.4392 - val_loss: 7.2693 - 120ms/epoch - 2ms/step\n",
      "Epoch 167/800\n",
      "78/78 - 0s - loss: 7.4447 - val_loss: 7.2364 - 125ms/epoch - 2ms/step\n",
      "Epoch 168/800\n",
      "78/78 - 0s - loss: 7.4469 - val_loss: 7.2446 - 122ms/epoch - 2ms/step\n",
      "Epoch 169/800\n",
      "78/78 - 0s - loss: 7.4177 - val_loss: 7.1948 - 118ms/epoch - 2ms/step\n",
      "Epoch 170/800\n",
      "78/78 - 0s - loss: 7.4092 - val_loss: 7.2733 - 120ms/epoch - 2ms/step\n",
      "Epoch 171/800\n",
      "78/78 - 0s - loss: 7.4575 - val_loss: 7.3479 - 124ms/epoch - 2ms/step\n",
      "Epoch 172/800\n",
      "78/78 - 0s - loss: 7.4967 - val_loss: 7.3733 - 169ms/epoch - 2ms/step\n",
      "Epoch 173/800\n",
      "78/78 - 0s - loss: 7.4821 - val_loss: 7.2840 - 204ms/epoch - 3ms/step\n",
      "Epoch 174/800\n",
      "78/78 - 0s - loss: 7.4564 - val_loss: 7.2619 - 285ms/epoch - 4ms/step\n",
      "Epoch 175/800\n",
      "78/78 - 0s - loss: 7.4356 - val_loss: 7.2328 - 330ms/epoch - 4ms/step\n",
      "Epoch 176/800\n",
      "78/78 - 0s - loss: 7.4132 - val_loss: 7.2301 - 266ms/epoch - 3ms/step\n",
      "Epoch 177/800\n",
      "78/78 - 0s - loss: 7.3875 - val_loss: 7.2406 - 256ms/epoch - 3ms/step\n",
      "Epoch 178/800\n",
      "78/78 - 0s - loss: 7.3892 - val_loss: 7.2046 - 214ms/epoch - 3ms/step\n",
      "Epoch 179/800\n",
      "78/78 - 0s - loss: 7.3659 - val_loss: 7.1684 - 155ms/epoch - 2ms/step\n",
      "Epoch 180/800\n",
      "78/78 - 0s - loss: 7.3934 - val_loss: 7.2001 - 198ms/epoch - 3ms/step\n",
      "Epoch 181/800\n",
      "78/78 - 0s - loss: 7.3887 - val_loss: 7.2163 - 231ms/epoch - 3ms/step\n",
      "Epoch 182/800\n",
      "78/78 - 0s - loss: 7.3761 - val_loss: 7.2069 - 164ms/epoch - 2ms/step\n",
      "Epoch 183/800\n",
      "78/78 - 0s - loss: 7.3796 - val_loss: 7.1990 - 169ms/epoch - 2ms/step\n",
      "Epoch 184/800\n",
      "78/78 - 0s - loss: 7.3619 - val_loss: 7.1755 - 153ms/epoch - 2ms/step\n",
      "Epoch 185/800\n",
      "78/78 - 0s - loss: 7.3616 - val_loss: 7.1732 - 178ms/epoch - 2ms/step\n",
      "Epoch 186/800\n",
      "78/78 - 0s - loss: 7.3529 - val_loss: 7.1603 - 162ms/epoch - 2ms/step\n",
      "Epoch 187/800\n",
      "78/78 - 0s - loss: 7.3663 - val_loss: 7.1563 - 155ms/epoch - 2ms/step\n",
      "Epoch 188/800\n",
      "78/78 - 0s - loss: 7.3656 - val_loss: 7.1973 - 182ms/epoch - 2ms/step\n",
      "Epoch 189/800\n",
      "78/78 - 0s - loss: 7.3499 - val_loss: 7.1864 - 159ms/epoch - 2ms/step\n",
      "Epoch 190/800\n",
      "78/78 - 0s - loss: 7.3533 - val_loss: 7.1973 - 141ms/epoch - 2ms/step\n",
      "Epoch 191/800\n",
      "78/78 - 0s - loss: 7.3795 - val_loss: 7.2417 - 147ms/epoch - 2ms/step\n",
      "Epoch 192/800\n",
      "78/78 - 0s - loss: 7.3794 - val_loss: 7.1956 - 166ms/epoch - 2ms/step\n",
      "Epoch 193/800\n",
      "78/78 - 0s - loss: 7.3528 - val_loss: 7.1651 - 142ms/epoch - 2ms/step\n",
      "Epoch 194/800\n",
      "78/78 - 0s - loss: 7.3270 - val_loss: 7.1696 - 158ms/epoch - 2ms/step\n",
      "Epoch 195/800\n",
      "78/78 - 0s - loss: 7.3386 - val_loss: 7.1683 - 160ms/epoch - 2ms/step\n",
      "Epoch 196/800\n",
      "78/78 - 0s - loss: 7.3710 - val_loss: 7.2172 - 125ms/epoch - 2ms/step\n",
      "Epoch 197/800\n",
      "78/78 - 0s - loss: 7.3691 - val_loss: 7.1969 - 119ms/epoch - 2ms/step\n",
      "Epoch 198/800\n",
      "78/78 - 0s - loss: 7.3585 - val_loss: 7.1308 - 119ms/epoch - 2ms/step\n",
      "Epoch 199/800\n",
      "78/78 - 0s - loss: 7.3579 - val_loss: 7.2097 - 136ms/epoch - 2ms/step\n",
      "Epoch 200/800\n",
      "78/78 - 0s - loss: 7.3457 - val_loss: 7.1241 - 118ms/epoch - 2ms/step\n",
      "Epoch 201/800\n",
      "78/78 - 0s - loss: 7.3079 - val_loss: 7.1201 - 136ms/epoch - 2ms/step\n",
      "Epoch 202/800\n",
      "78/78 - 0s - loss: 7.3222 - val_loss: 7.1111 - 115ms/epoch - 1ms/step\n",
      "Epoch 203/800\n",
      "78/78 - 0s - loss: 7.2994 - val_loss: 7.1222 - 118ms/epoch - 2ms/step\n",
      "Epoch 204/800\n",
      "78/78 - 0s - loss: 7.2957 - val_loss: 7.0817 - 113ms/epoch - 1ms/step\n",
      "Epoch 205/800\n",
      "78/78 - 0s - loss: 7.3192 - val_loss: 7.1826 - 103ms/epoch - 1ms/step\n",
      "Epoch 206/800\n",
      "78/78 - 0s - loss: 7.3204 - val_loss: 7.1372 - 122ms/epoch - 2ms/step\n",
      "Epoch 207/800\n",
      "78/78 - 0s - loss: 7.3010 - val_loss: 7.0794 - 108ms/epoch - 1ms/step\n",
      "Epoch 208/800\n",
      "78/78 - 0s - loss: 7.2880 - val_loss: 7.1461 - 118ms/epoch - 2ms/step\n",
      "Epoch 209/800\n",
      "78/78 - 0s - loss: 7.3018 - val_loss: 7.1168 - 124ms/epoch - 2ms/step\n",
      "Epoch 210/800\n",
      "78/78 - 0s - loss: 7.2679 - val_loss: 7.1298 - 117ms/epoch - 1ms/step\n",
      "Epoch 211/800\n",
      "78/78 - 0s - loss: 7.2765 - val_loss: 7.1252 - 109ms/epoch - 1ms/step\n",
      "Epoch 212/800\n",
      "78/78 - 0s - loss: 7.2641 - val_loss: 7.0708 - 116ms/epoch - 1ms/step\n",
      "Epoch 213/800\n",
      "78/78 - 0s - loss: 7.2334 - val_loss: 7.1000 - 117ms/epoch - 1ms/step\n",
      "Epoch 214/800\n",
      "78/78 - 0s - loss: 7.2578 - val_loss: 7.1200 - 119ms/epoch - 2ms/step\n",
      "Epoch 215/800\n",
      "78/78 - 0s - loss: 7.2590 - val_loss: 7.1345 - 115ms/epoch - 1ms/step\n",
      "Epoch 216/800\n",
      "78/78 - 0s - loss: 7.2823 - val_loss: 7.0884 - 114ms/epoch - 1ms/step\n",
      "Epoch 217/800\n",
      "78/78 - 0s - loss: 7.2609 - val_loss: 7.1561 - 102ms/epoch - 1ms/step\n",
      "Epoch 218/800\n",
      "78/78 - 0s - loss: 7.2533 - val_loss: 7.0725 - 118ms/epoch - 2ms/step\n",
      "Epoch 219/800\n",
      "78/78 - 0s - loss: 7.2557 - val_loss: 7.0574 - 114ms/epoch - 1ms/step\n",
      "Epoch 220/800\n",
      "78/78 - 0s - loss: 7.2806 - val_loss: 7.1607 - 119ms/epoch - 2ms/step\n",
      "Epoch 221/800\n",
      "78/78 - 0s - loss: 7.2820 - val_loss: 7.1218 - 122ms/epoch - 2ms/step\n",
      "Epoch 222/800\n",
      "78/78 - 0s - loss: 7.2986 - val_loss: 7.1203 - 112ms/epoch - 1ms/step\n",
      "Epoch 223/800\n",
      "78/78 - 0s - loss: 7.2480 - val_loss: 7.0408 - 145ms/epoch - 2ms/step\n",
      "Epoch 224/800\n",
      "78/78 - 0s - loss: 7.2267 - val_loss: 7.0534 - 121ms/epoch - 2ms/step\n",
      "Epoch 225/800\n",
      "78/78 - 0s - loss: 7.2671 - val_loss: 7.0124 - 135ms/epoch - 2ms/step\n",
      "Epoch 226/800\n",
      "78/78 - 0s - loss: 7.2244 - val_loss: 7.1084 - 110ms/epoch - 1ms/step\n",
      "Epoch 227/800\n",
      "78/78 - 0s - loss: 7.2417 - val_loss: 7.0856 - 116ms/epoch - 1ms/step\n",
      "Epoch 228/800\n",
      "78/78 - 0s - loss: 7.2325 - val_loss: 7.0469 - 122ms/epoch - 2ms/step\n",
      "Epoch 229/800\n",
      "78/78 - 0s - loss: 7.2088 - val_loss: 7.0295 - 120ms/epoch - 2ms/step\n",
      "Epoch 230/800\n",
      "78/78 - 0s - loss: 7.2270 - val_loss: 7.1314 - 123ms/epoch - 2ms/step\n",
      "Epoch 231/800\n",
      "78/78 - 0s - loss: 7.2590 - val_loss: 7.0516 - 135ms/epoch - 2ms/step\n",
      "Epoch 232/800\n",
      "78/78 - 0s - loss: 7.2523 - val_loss: 7.0311 - 122ms/epoch - 2ms/step\n",
      "Epoch 233/800\n",
      "78/78 - 0s - loss: 7.2446 - val_loss: 7.0450 - 133ms/epoch - 2ms/step\n",
      "Epoch 234/800\n",
      "78/78 - 0s - loss: 7.2049 - val_loss: 7.0130 - 124ms/epoch - 2ms/step\n",
      "Epoch 235/800\n",
      "78/78 - 0s - loss: 7.1725 - val_loss: 6.9823 - 121ms/epoch - 2ms/step\n",
      "Epoch 236/800\n",
      "78/78 - 0s - loss: 7.1723 - val_loss: 7.0233 - 138ms/epoch - 2ms/step\n",
      "Epoch 237/800\n",
      "78/78 - 0s - loss: 7.1925 - val_loss: 7.0168 - 106ms/epoch - 1ms/step\n",
      "Epoch 238/800\n",
      "78/78 - 0s - loss: 7.1895 - val_loss: 7.0286 - 124ms/epoch - 2ms/step\n",
      "Epoch 239/800\n",
      "78/78 - 0s - loss: 7.1899 - val_loss: 7.0363 - 136ms/epoch - 2ms/step\n",
      "Epoch 240/800\n",
      "78/78 - 0s - loss: 7.2099 - val_loss: 7.0428 - 106ms/epoch - 1ms/step\n",
      "Epoch 241/800\n",
      "78/78 - 0s - loss: 7.1989 - val_loss: 7.0212 - 119ms/epoch - 2ms/step\n",
      "Epoch 242/800\n",
      "78/78 - 0s - loss: 7.1824 - val_loss: 6.9952 - 135ms/epoch - 2ms/step\n",
      "Epoch 243/800\n",
      "78/78 - 0s - loss: 7.1899 - val_loss: 6.9880 - 109ms/epoch - 1ms/step\n",
      "Epoch 244/800\n",
      "78/78 - 0s - loss: 7.1968 - val_loss: 7.0526 - 120ms/epoch - 2ms/step\n",
      "Epoch 245/800\n",
      "78/78 - 0s - loss: 7.2409 - val_loss: 7.1024 - 130ms/epoch - 2ms/step\n",
      "Epoch 246/800\n",
      "78/78 - 0s - loss: 7.2023 - val_loss: 7.0485 - 102ms/epoch - 1ms/step\n",
      "Epoch 247/800\n",
      "78/78 - 0s - loss: 7.2023 - val_loss: 7.0066 - 114ms/epoch - 1ms/step\n",
      "Epoch 248/800\n",
      "78/78 - 0s - loss: 7.1795 - val_loss: 6.9877 - 134ms/epoch - 2ms/step\n",
      "Epoch 249/800\n",
      "78/78 - 0s - loss: 7.1685 - val_loss: 7.0257 - 130ms/epoch - 2ms/step\n",
      "Epoch 250/800\n",
      "78/78 - 0s - loss: 7.1716 - val_loss: 6.9965 - 116ms/epoch - 1ms/step\n",
      "Epoch 251/800\n",
      "78/78 - 0s - loss: 7.1677 - val_loss: 6.9805 - 115ms/epoch - 1ms/step\n",
      "Epoch 252/800\n",
      "78/78 - 0s - loss: 7.1860 - val_loss: 6.9984 - 119ms/epoch - 2ms/step\n",
      "Epoch 253/800\n",
      "78/78 - 0s - loss: 7.1844 - val_loss: 7.0516 - 101ms/epoch - 1ms/step\n",
      "Epoch 254/800\n",
      "78/78 - 0s - loss: 7.1766 - val_loss: 7.0105 - 131ms/epoch - 2ms/step\n",
      "Epoch 255/800\n",
      "78/78 - 0s - loss: 7.1610 - val_loss: 6.9265 - 120ms/epoch - 2ms/step\n",
      "Epoch 256/800\n",
      "78/78 - 0s - loss: 7.1463 - val_loss: 6.9626 - 114ms/epoch - 1ms/step\n",
      "Epoch 257/800\n",
      "78/78 - 0s - loss: 7.1362 - val_loss: 6.9437 - 117ms/epoch - 2ms/step\n",
      "Epoch 258/800\n",
      "78/78 - 0s - loss: 7.1426 - val_loss: 6.9576 - 115ms/epoch - 1ms/step\n",
      "Epoch 259/800\n",
      "78/78 - 0s - loss: 7.1867 - val_loss: 7.0865 - 130ms/epoch - 2ms/step\n",
      "Epoch 260/800\n",
      "78/78 - 0s - loss: 7.2106 - val_loss: 6.9931 - 119ms/epoch - 2ms/step\n",
      "Epoch 261/800\n",
      "78/78 - 0s - loss: 7.1944 - val_loss: 7.0495 - 127ms/epoch - 2ms/step\n",
      "Epoch 262/800\n",
      "78/78 - 0s - loss: 7.1967 - val_loss: 7.0143 - 115ms/epoch - 1ms/step\n",
      "Epoch 263/800\n",
      "78/78 - 0s - loss: 7.1759 - val_loss: 6.9823 - 124ms/epoch - 2ms/step\n",
      "Epoch 264/800\n",
      "78/78 - 0s - loss: 7.1726 - val_loss: 7.0073 - 116ms/epoch - 1ms/step\n",
      "Epoch 265/800\n",
      "78/78 - 0s - loss: 7.1541 - val_loss: 6.9744 - 128ms/epoch - 2ms/step\n",
      "Epoch 266/800\n",
      "78/78 - 0s - loss: 7.1416 - val_loss: 6.9933 - 115ms/epoch - 1ms/step\n",
      "Epoch 267/800\n",
      "78/78 - 0s - loss: 7.1277 - val_loss: 6.9670 - 131ms/epoch - 2ms/step\n",
      "Epoch 268/800\n",
      "78/78 - 0s - loss: 7.1283 - val_loss: 6.9938 - 115ms/epoch - 1ms/step\n",
      "Epoch 269/800\n",
      "78/78 - 0s - loss: 7.1240 - val_loss: 7.0006 - 115ms/epoch - 1ms/step\n",
      "Epoch 270/800\n",
      "78/78 - 0s - loss: 7.1366 - val_loss: 6.9623 - 120ms/epoch - 2ms/step\n",
      "Epoch 271/800\n",
      "78/78 - 0s - loss: 7.1364 - val_loss: 6.9291 - 163ms/epoch - 2ms/step\n",
      "Epoch 272/800\n",
      "78/78 - 0s - loss: 7.1564 - val_loss: 6.9560 - 115ms/epoch - 1ms/step\n",
      "Epoch 273/800\n",
      "78/78 - 0s - loss: 7.1359 - val_loss: 7.0010 - 109ms/epoch - 1ms/step\n",
      "Epoch 274/800\n",
      "78/78 - 0s - loss: 7.1293 - val_loss: 6.9332 - 115ms/epoch - 1ms/step\n",
      "Epoch 275/800\n",
      "78/78 - 0s - loss: 7.1200 - val_loss: 6.9563 - 117ms/epoch - 1ms/step\n",
      "Epoch 276/800\n",
      "78/78 - 0s - loss: 7.1573 - val_loss: 6.9417 - 123ms/epoch - 2ms/step\n",
      "Epoch 277/800\n",
      "78/78 - 0s - loss: 7.1350 - val_loss: 7.0118 - 109ms/epoch - 1ms/step\n",
      "Epoch 278/800\n",
      "78/78 - 0s - loss: 7.1035 - val_loss: 6.9456 - 103ms/epoch - 1ms/step\n",
      "Epoch 279/800\n",
      "78/78 - 0s - loss: 7.1126 - val_loss: 6.9559 - 114ms/epoch - 1ms/step\n",
      "Epoch 280/800\n",
      "78/78 - 0s - loss: 7.1020 - val_loss: 7.0064 - 116ms/epoch - 1ms/step\n",
      "Epoch 281/800\n",
      "78/78 - 0s - loss: 7.1086 - val_loss: 6.9201 - 114ms/epoch - 1ms/step\n",
      "Epoch 282/800\n",
      "78/78 - 0s - loss: 7.0873 - val_loss: 6.9487 - 115ms/epoch - 1ms/step\n",
      "Epoch 283/800\n",
      "78/78 - 0s - loss: 7.0878 - val_loss: 6.9051 - 102ms/epoch - 1ms/step\n",
      "Epoch 284/800\n",
      "78/78 - 0s - loss: 7.0854 - val_loss: 6.9223 - 119ms/epoch - 2ms/step\n",
      "Epoch 285/800\n",
      "78/78 - 0s - loss: 7.1152 - val_loss: 6.9692 - 116ms/epoch - 1ms/step\n",
      "Epoch 286/800\n",
      "78/78 - 0s - loss: 7.0964 - val_loss: 6.9159 - 117ms/epoch - 2ms/step\n",
      "Epoch 287/800\n",
      "78/78 - 0s - loss: 7.0898 - val_loss: 6.9185 - 99ms/epoch - 1ms/step\n",
      "Epoch 288/800\n",
      "78/78 - 0s - loss: 7.1239 - val_loss: 6.9745 - 118ms/epoch - 2ms/step\n",
      "Epoch 289/800\n",
      "78/78 - 0s - loss: 7.1197 - val_loss: 7.0187 - 115ms/epoch - 1ms/step\n",
      "Epoch 290/800\n",
      "78/78 - 0s - loss: 7.1407 - val_loss: 7.0077 - 119ms/epoch - 2ms/step\n",
      "Epoch 291/800\n",
      "78/78 - 0s - loss: 7.1386 - val_loss: 7.0399 - 118ms/epoch - 2ms/step\n",
      "Epoch 292/800\n",
      "78/78 - 0s - loss: 7.1593 - val_loss: 6.9588 - 113ms/epoch - 1ms/step\n",
      "Epoch 293/800\n",
      "78/78 - 0s - loss: 7.1291 - val_loss: 6.9093 - 117ms/epoch - 1ms/step\n",
      "Epoch 294/800\n",
      "78/78 - 0s - loss: 7.1224 - val_loss: 6.9177 - 100ms/epoch - 1ms/step\n",
      "Epoch 295/800\n",
      "78/78 - 0s - loss: 7.1100 - val_loss: 6.9470 - 117ms/epoch - 1ms/step\n",
      "Epoch 296/800\n",
      "78/78 - 0s - loss: 7.0838 - val_loss: 6.9516 - 119ms/epoch - 2ms/step\n",
      "Epoch 297/800\n",
      "78/78 - 0s - loss: 7.0519 - val_loss: 6.9128 - 116ms/epoch - 1ms/step\n",
      "Epoch 298/800\n",
      "78/78 - 0s - loss: 7.0394 - val_loss: 6.8793 - 114ms/epoch - 1ms/step\n",
      "Epoch 299/800\n",
      "78/78 - 0s - loss: 7.0817 - val_loss: 6.9053 - 117ms/epoch - 1ms/step\n",
      "Epoch 300/800\n",
      "78/78 - 0s - loss: 7.0745 - val_loss: 6.8855 - 103ms/epoch - 1ms/step\n",
      "Epoch 301/800\n",
      "78/78 - 0s - loss: 7.0572 - val_loss: 6.8888 - 115ms/epoch - 1ms/step\n",
      "Epoch 302/800\n",
      "78/78 - 0s - loss: 7.0957 - val_loss: 6.9170 - 118ms/epoch - 2ms/step\n",
      "Epoch 303/800\n",
      "78/78 - 0s - loss: 7.1147 - val_loss: 6.9461 - 113ms/epoch - 1ms/step\n",
      "Epoch 304/800\n",
      "78/78 - 0s - loss: 7.0624 - val_loss: 6.8894 - 127ms/epoch - 2ms/step\n",
      "Epoch 305/800\n",
      "78/78 - 0s - loss: 7.0820 - val_loss: 6.9460 - 117ms/epoch - 1ms/step\n",
      "Epoch 306/800\n",
      "78/78 - 0s - loss: 7.1366 - val_loss: 6.9861 - 115ms/epoch - 1ms/step\n",
      "Epoch 307/800\n",
      "78/78 - 0s - loss: 7.1332 - val_loss: 6.9920 - 160ms/epoch - 2ms/step\n",
      "Epoch 308/800\n",
      "78/78 - 0s - loss: 7.0990 - val_loss: 6.8961 - 135ms/epoch - 2ms/step\n",
      "Epoch 309/800\n",
      "78/78 - 0s - loss: 7.0790 - val_loss: 6.8679 - 114ms/epoch - 1ms/step\n",
      "Epoch 310/800\n",
      "78/78 - 0s - loss: 7.0287 - val_loss: 6.8468 - 116ms/epoch - 1ms/step\n",
      "Epoch 311/800\n",
      "78/78 - 0s - loss: 7.0538 - val_loss: 6.8965 - 115ms/epoch - 1ms/step\n",
      "Epoch 312/800\n",
      "78/78 - 0s - loss: 7.0770 - val_loss: 6.8709 - 117ms/epoch - 2ms/step\n",
      "Epoch 313/800\n",
      "78/78 - 0s - loss: 7.0593 - val_loss: 6.8749 - 113ms/epoch - 1ms/step\n",
      "Epoch 314/800\n",
      "78/78 - 0s - loss: 7.0514 - val_loss: 6.8968 - 102ms/epoch - 1ms/step\n",
      "Epoch 315/800\n",
      "78/78 - 0s - loss: 7.0502 - val_loss: 6.9149 - 118ms/epoch - 2ms/step\n",
      "Epoch 316/800\n",
      "78/78 - 0s - loss: 7.0377 - val_loss: 6.8474 - 114ms/epoch - 1ms/step\n",
      "Epoch 317/800\n",
      "78/78 - 0s - loss: 7.0244 - val_loss: 6.8854 - 120ms/epoch - 2ms/step\n",
      "Epoch 318/800\n",
      "78/78 - 0s - loss: 7.0727 - val_loss: 6.8865 - 115ms/epoch - 1ms/step\n",
      "Epoch 319/800\n",
      "78/78 - 0s - loss: 7.0465 - val_loss: 6.8964 - 114ms/epoch - 1ms/step\n",
      "Epoch 320/800\n",
      "78/78 - 0s - loss: 7.0550 - val_loss: 6.9048 - 116ms/epoch - 1ms/step\n",
      "Epoch 321/800\n",
      "78/78 - 0s - loss: 7.0669 - val_loss: 6.8910 - 103ms/epoch - 1ms/step\n",
      "Epoch 322/800\n",
      "78/78 - 0s - loss: 7.0634 - val_loss: 6.8991 - 130ms/epoch - 2ms/step\n",
      "Epoch 323/800\n",
      "78/78 - 0s - loss: 7.0237 - val_loss: 6.8969 - 104ms/epoch - 1ms/step\n",
      "Epoch 324/800\n",
      "78/78 - 0s - loss: 7.0064 - val_loss: 6.8726 - 116ms/epoch - 1ms/step\n",
      "Epoch 325/800\n",
      "78/78 - 0s - loss: 7.0279 - val_loss: 6.9321 - 118ms/epoch - 2ms/step\n",
      "Epoch 326/800\n",
      "78/78 - 0s - loss: 7.0367 - val_loss: 6.8706 - 114ms/epoch - 1ms/step\n",
      "Epoch 327/800\n",
      "78/78 - 0s - loss: 7.0437 - val_loss: 6.8898 - 119ms/epoch - 2ms/step\n",
      "Epoch 328/800\n",
      "78/78 - 0s - loss: 7.0466 - val_loss: 6.8794 - 115ms/epoch - 1ms/step\n",
      "Epoch 329/800\n",
      "78/78 - 0s - loss: 7.0475 - val_loss: 6.8856 - 98ms/epoch - 1ms/step\n",
      "Epoch 330/800\n",
      "78/78 - 0s - loss: 7.0384 - val_loss: 6.8974 - 119ms/epoch - 2ms/step\n",
      "Epoch 331/800\n",
      "78/78 - 0s - loss: 7.0371 - val_loss: 6.8889 - 113ms/epoch - 1ms/step\n",
      "Epoch 332/800\n",
      "78/78 - 0s - loss: 7.0220 - val_loss: 6.8949 - 123ms/epoch - 2ms/step\n",
      "Epoch 333/800\n",
      "78/78 - 0s - loss: 7.0292 - val_loss: 6.9375 - 112ms/epoch - 1ms/step\n",
      "Epoch 334/800\n",
      "78/78 - 0s - loss: 7.0252 - val_loss: 6.8785 - 116ms/epoch - 1ms/step\n",
      "Epoch 335/800\n",
      "78/78 - 0s - loss: 7.0251 - val_loss: 6.8497 - 114ms/epoch - 1ms/step\n",
      "Epoch 336/800\n",
      "78/78 - 0s - loss: 7.0348 - val_loss: 6.8960 - 102ms/epoch - 1ms/step\n",
      "Epoch 337/800\n",
      "78/78 - 0s - loss: 7.0683 - val_loss: 6.9851 - 118ms/epoch - 2ms/step\n",
      "Epoch 338/800\n",
      "78/78 - 0s - loss: 7.0832 - val_loss: 6.9038 - 113ms/epoch - 1ms/step\n",
      "Epoch 339/800\n",
      "78/78 - 0s - loss: 7.0931 - val_loss: 6.8974 - 130ms/epoch - 2ms/step\n",
      "Epoch 340/800\n",
      "78/78 - 0s - loss: 7.0247 - val_loss: 6.8776 - 102ms/epoch - 1ms/step\n",
      "Epoch 341/800\n",
      "78/78 - 0s - loss: 6.9980 - val_loss: 6.8446 - 125ms/epoch - 2ms/step\n",
      "Epoch 342/800\n",
      "78/78 - 0s - loss: 6.9913 - val_loss: 6.8370 - 114ms/epoch - 1ms/step\n",
      "Epoch 343/800\n",
      "78/78 - 0s - loss: 6.9790 - val_loss: 6.8229 - 111ms/epoch - 1ms/step\n",
      "Epoch 344/800\n",
      "78/78 - 0s - loss: 6.9882 - val_loss: 6.8733 - 122ms/epoch - 2ms/step\n",
      "Epoch 345/800\n",
      "78/78 - 0s - loss: 7.0132 - val_loss: 6.8368 - 116ms/epoch - 1ms/step\n",
      "Epoch 346/800\n",
      "78/78 - 0s - loss: 6.9959 - val_loss: 6.8026 - 118ms/epoch - 2ms/step\n",
      "Epoch 347/800\n",
      "78/78 - 0s - loss: 7.0048 - val_loss: 6.8950 - 107ms/epoch - 1ms/step\n",
      "Epoch 348/800\n",
      "78/78 - 0s - loss: 7.0004 - val_loss: 6.8462 - 141ms/epoch - 2ms/step\n",
      "Epoch 349/800\n",
      "78/78 - 0s - loss: 6.9844 - val_loss: 6.7904 - 127ms/epoch - 2ms/step\n",
      "Epoch 350/800\n",
      "78/78 - 0s - loss: 6.9881 - val_loss: 6.8190 - 146ms/epoch - 2ms/step\n",
      "Epoch 351/800\n",
      "78/78 - 0s - loss: 7.0146 - val_loss: 6.8269 - 149ms/epoch - 2ms/step\n",
      "Epoch 352/800\n",
      "78/78 - 0s - loss: 7.0212 - val_loss: 6.8773 - 133ms/epoch - 2ms/step\n",
      "Epoch 353/800\n",
      "78/78 - 0s - loss: 7.0206 - val_loss: 6.8620 - 167ms/epoch - 2ms/step\n",
      "Epoch 354/800\n",
      "78/78 - 0s - loss: 6.9808 - val_loss: 6.8346 - 148ms/epoch - 2ms/step\n",
      "Epoch 355/800\n",
      "78/78 - 0s - loss: 6.9805 - val_loss: 6.7969 - 127ms/epoch - 2ms/step\n",
      "Epoch 356/800\n",
      "78/78 - 0s - loss: 6.9849 - val_loss: 6.8683 - 119ms/epoch - 2ms/step\n",
      "Epoch 357/800\n",
      "78/78 - 0s - loss: 7.0006 - val_loss: 6.8577 - 103ms/epoch - 1ms/step\n",
      "Epoch 358/800\n",
      "78/78 - 0s - loss: 6.9805 - val_loss: 6.8384 - 119ms/epoch - 2ms/step\n",
      "Epoch 359/800\n",
      "78/78 - 0s - loss: 6.9975 - val_loss: 6.8720 - 117ms/epoch - 1ms/step\n",
      "Epoch 360/800\n",
      "78/78 - 0s - loss: 7.0207 - val_loss: 6.9062 - 128ms/epoch - 2ms/step\n",
      "Epoch 361/800\n",
      "78/78 - 0s - loss: 7.0216 - val_loss: 6.8577 - 123ms/epoch - 2ms/step\n",
      "Epoch 362/800\n",
      "78/78 - 0s - loss: 7.0188 - val_loss: 6.8310 - 119ms/epoch - 2ms/step\n",
      "Epoch 363/800\n",
      "78/78 - 0s - loss: 7.0238 - val_loss: 6.8681 - 118ms/epoch - 2ms/step\n",
      "Epoch 364/800\n",
      "78/78 - 0s - loss: 7.0242 - val_loss: 6.8213 - 134ms/epoch - 2ms/step\n",
      "Epoch 365/800\n",
      "78/78 - 0s - loss: 6.9720 - val_loss: 6.7712 - 119ms/epoch - 2ms/step\n",
      "Epoch 366/800\n",
      "78/78 - 0s - loss: 6.9679 - val_loss: 6.8007 - 117ms/epoch - 1ms/step\n",
      "Epoch 367/800\n",
      "78/78 - 0s - loss: 6.9798 - val_loss: 6.8548 - 103ms/epoch - 1ms/step\n",
      "Epoch 368/800\n",
      "78/78 - 0s - loss: 6.9890 - val_loss: 6.7986 - 124ms/epoch - 2ms/step\n",
      "Epoch 369/800\n",
      "78/78 - 0s - loss: 6.9853 - val_loss: 6.8173 - 115ms/epoch - 1ms/step\n",
      "Epoch 370/800\n",
      "78/78 - 0s - loss: 7.0125 - val_loss: 6.8329 - 115ms/epoch - 1ms/step\n",
      "Epoch 371/800\n",
      "78/78 - 0s - loss: 6.9770 - val_loss: 6.8082 - 110ms/epoch - 1ms/step\n",
      "Epoch 372/800\n",
      "78/78 - 0s - loss: 7.0024 - val_loss: 6.7781 - 120ms/epoch - 2ms/step\n",
      "Epoch 373/800\n",
      "78/78 - 0s - loss: 6.9778 - val_loss: 6.8300 - 114ms/epoch - 1ms/step\n",
      "Epoch 374/800\n",
      "78/78 - 0s - loss: 6.9760 - val_loss: 6.8033 - 118ms/epoch - 2ms/step\n",
      "Epoch 375/800\n",
      "78/78 - 0s - loss: 6.9731 - val_loss: 6.8372 - 115ms/epoch - 1ms/step\n",
      "Epoch 376/800\n",
      "78/78 - 0s - loss: 6.9641 - val_loss: 6.8391 - 117ms/epoch - 1ms/step\n",
      "Epoch 377/800\n",
      "78/78 - 0s - loss: 6.9573 - val_loss: 6.7961 - 114ms/epoch - 1ms/step\n",
      "Epoch 378/800\n",
      "78/78 - 0s - loss: 6.9583 - val_loss: 6.8395 - 104ms/epoch - 1ms/step\n",
      "Epoch 379/800\n",
      "78/78 - 0s - loss: 6.9498 - val_loss: 6.7944 - 117ms/epoch - 1ms/step\n",
      "Epoch 380/800\n",
      "78/78 - 0s - loss: 6.9575 - val_loss: 6.7782 - 127ms/epoch - 2ms/step\n",
      "Epoch 381/800\n",
      "78/78 - 0s - loss: 6.9561 - val_loss: 6.7921 - 152ms/epoch - 2ms/step\n",
      "Epoch 382/800\n",
      "78/78 - 0s - loss: 6.9748 - val_loss: 6.8115 - 130ms/epoch - 2ms/step\n",
      "Epoch 383/800\n",
      "78/78 - 0s - loss: 6.9850 - val_loss: 6.7896 - 127ms/epoch - 2ms/step\n",
      "Epoch 384/800\n",
      "78/78 - 0s - loss: 6.9603 - val_loss: 6.8202 - 122ms/epoch - 2ms/step\n",
      "Epoch 385/800\n",
      "78/78 - 0s - loss: 6.9386 - val_loss: 6.7961 - 129ms/epoch - 2ms/step\n",
      "Epoch 386/800\n",
      "78/78 - 0s - loss: 6.9828 - val_loss: 6.8005 - 123ms/epoch - 2ms/step\n",
      "Epoch 387/800\n",
      "78/78 - 0s - loss: 6.9899 - val_loss: 6.8517 - 124ms/epoch - 2ms/step\n",
      "Epoch 388/800\n",
      "78/78 - 0s - loss: 6.9874 - val_loss: 6.8356 - 118ms/epoch - 2ms/step\n",
      "Epoch 389/800\n",
      "78/78 - 0s - loss: 6.9683 - val_loss: 6.7997 - 109ms/epoch - 1ms/step\n",
      "Epoch 390/800\n",
      "78/78 - 0s - loss: 6.9423 - val_loss: 6.7601 - 113ms/epoch - 1ms/step\n",
      "Epoch 391/800\n",
      "78/78 - 0s - loss: 6.9343 - val_loss: 6.8351 - 139ms/epoch - 2ms/step\n",
      "Epoch 392/800\n",
      "78/78 - 0s - loss: 6.9364 - val_loss: 6.7958 - 127ms/epoch - 2ms/step\n",
      "Epoch 393/800\n",
      "78/78 - 0s - loss: 6.9601 - val_loss: 6.7770 - 126ms/epoch - 2ms/step\n",
      "Epoch 394/800\n",
      "78/78 - 0s - loss: 6.9739 - val_loss: 6.7955 - 131ms/epoch - 2ms/step\n",
      "Epoch 395/800\n",
      "78/78 - 0s - loss: 6.9648 - val_loss: 6.7675 - 128ms/epoch - 2ms/step\n",
      "Epoch 396/800\n",
      "78/78 - 0s - loss: 6.9361 - val_loss: 6.7972 - 118ms/epoch - 2ms/step\n",
      "Epoch 397/800\n",
      "78/78 - 0s - loss: 6.9487 - val_loss: 6.8435 - 115ms/epoch - 1ms/step\n",
      "Epoch 398/800\n",
      "78/78 - 0s - loss: 6.9535 - val_loss: 6.7978 - 134ms/epoch - 2ms/step\n",
      "Epoch 399/800\n",
      "78/78 - 0s - loss: 6.9825 - val_loss: 6.8156 - 125ms/epoch - 2ms/step\n",
      "Epoch 400/800\n",
      "78/78 - 0s - loss: 6.9362 - val_loss: 6.8569 - 101ms/epoch - 1ms/step\n",
      "Epoch 401/800\n",
      "78/78 - 0s - loss: 6.9719 - val_loss: 6.7894 - 104ms/epoch - 1ms/step\n",
      "Epoch 402/800\n",
      "78/78 - 0s - loss: 6.9218 - val_loss: 6.7746 - 114ms/epoch - 1ms/step\n",
      "Epoch 403/800\n",
      "78/78 - 0s - loss: 6.8907 - val_loss: 6.7353 - 121ms/epoch - 2ms/step\n",
      "Epoch 404/800\n",
      "78/78 - 0s - loss: 6.8887 - val_loss: 6.7218 - 122ms/epoch - 2ms/step\n",
      "Epoch 405/800\n",
      "78/78 - 0s - loss: 6.9217 - val_loss: 6.7787 - 123ms/epoch - 2ms/step\n",
      "Epoch 406/800\n",
      "78/78 - 0s - loss: 6.9527 - val_loss: 6.8649 - 131ms/epoch - 2ms/step\n",
      "Epoch 407/800\n",
      "78/78 - 0s - loss: 6.9828 - val_loss: 6.7779 - 118ms/epoch - 2ms/step\n",
      "Epoch 408/800\n",
      "78/78 - 0s - loss: 6.9429 - val_loss: 6.7762 - 116ms/epoch - 1ms/step\n",
      "Epoch 409/800\n",
      "78/78 - 0s - loss: 6.9070 - val_loss: 6.7755 - 102ms/epoch - 1ms/step\n",
      "Epoch 410/800\n",
      "78/78 - 0s - loss: 6.9239 - val_loss: 6.8021 - 125ms/epoch - 2ms/step\n",
      "Epoch 411/800\n",
      "78/78 - 0s - loss: 6.9573 - val_loss: 6.7805 - 124ms/epoch - 2ms/step\n",
      "Epoch 412/800\n",
      "78/78 - 0s - loss: 6.9193 - val_loss: 6.7905 - 109ms/epoch - 1ms/step\n",
      "Epoch 413/800\n",
      "78/78 - 0s - loss: 6.9061 - val_loss: 6.7197 - 132ms/epoch - 2ms/step\n",
      "Epoch 414/800\n",
      "78/78 - 0s - loss: 6.9086 - val_loss: 6.7615 - 124ms/epoch - 2ms/step\n",
      "Epoch 415/800\n",
      "78/78 - 0s - loss: 6.9348 - val_loss: 6.7673 - 119ms/epoch - 2ms/step\n",
      "Epoch 416/800\n",
      "78/78 - 0s - loss: 6.9472 - val_loss: 6.7464 - 121ms/epoch - 2ms/step\n",
      "Epoch 417/800\n",
      "78/78 - 0s - loss: 6.8971 - val_loss: 6.7948 - 121ms/epoch - 2ms/step\n",
      "Epoch 418/800\n",
      "78/78 - 0s - loss: 6.9372 - val_loss: 6.7857 - 118ms/epoch - 2ms/step\n",
      "Epoch 419/800\n",
      "78/78 - 0s - loss: 6.9369 - val_loss: 6.8233 - 121ms/epoch - 2ms/step\n",
      "Epoch 420/800\n",
      "78/78 - 0s - loss: 6.9287 - val_loss: 6.7493 - 122ms/epoch - 2ms/step\n",
      "Epoch 421/800\n",
      "78/78 - 0s - loss: 6.9086 - val_loss: 6.7551 - 120ms/epoch - 2ms/step\n",
      "Epoch 422/800\n",
      "78/78 - 0s - loss: 6.9191 - val_loss: 6.7551 - 120ms/epoch - 2ms/step\n",
      "Epoch 423/800\n",
      "78/78 - 0s - loss: 6.9331 - val_loss: 6.7468 - 150ms/epoch - 2ms/step\n",
      "Epoch 424/800\n",
      "78/78 - 0s - loss: 6.9121 - val_loss: 6.7474 - 124ms/epoch - 2ms/step\n",
      "Epoch 425/800\n",
      "78/78 - 0s - loss: 6.9358 - val_loss: 6.7770 - 101ms/epoch - 1ms/step\n",
      "Epoch 426/800\n",
      "78/78 - 0s - loss: 6.9329 - val_loss: 6.7796 - 97ms/epoch - 1ms/step\n",
      "Epoch 427/800\n",
      "78/78 - 0s - loss: 6.9169 - val_loss: 6.8019 - 118ms/epoch - 2ms/step\n",
      "Epoch 428/800\n",
      "78/78 - 0s - loss: 6.9096 - val_loss: 6.7891 - 123ms/epoch - 2ms/step\n",
      "Epoch 429/800\n",
      "78/78 - 0s - loss: 6.9133 - val_loss: 6.7359 - 128ms/epoch - 2ms/step\n",
      "Epoch 430/800\n",
      "78/78 - 0s - loss: 6.9202 - val_loss: 6.7295 - 121ms/epoch - 2ms/step\n",
      "Epoch 431/800\n",
      "78/78 - 0s - loss: 6.9170 - val_loss: 6.7825 - 128ms/epoch - 2ms/step\n",
      "Epoch 432/800\n",
      "78/78 - 0s - loss: 6.9314 - val_loss: 6.7548 - 121ms/epoch - 2ms/step\n",
      "Epoch 433/800\n",
      "78/78 - 0s - loss: 6.9404 - val_loss: 6.7724 - 119ms/epoch - 2ms/step\n",
      "Epoch 434/800\n",
      "78/78 - 0s - loss: 6.9190 - val_loss: 6.7656 - 120ms/epoch - 2ms/step\n",
      "Epoch 435/800\n",
      "78/78 - 0s - loss: 6.8863 - val_loss: 6.7055 - 120ms/epoch - 2ms/step\n",
      "Epoch 436/800\n",
      "78/78 - 0s - loss: 6.8572 - val_loss: 6.7041 - 163ms/epoch - 2ms/step\n",
      "Epoch 437/800\n",
      "78/78 - 0s - loss: 6.8769 - val_loss: 6.7083 - 145ms/epoch - 2ms/step\n",
      "Epoch 438/800\n",
      "78/78 - 0s - loss: 6.8781 - val_loss: 6.7205 - 130ms/epoch - 2ms/step\n",
      "Epoch 439/800\n",
      "78/78 - 0s - loss: 6.8887 - val_loss: 6.7066 - 136ms/epoch - 2ms/step\n",
      "Epoch 440/800\n",
      "78/78 - 0s - loss: 6.9092 - val_loss: 6.7726 - 123ms/epoch - 2ms/step\n",
      "Epoch 441/800\n",
      "78/78 - 0s - loss: 6.9332 - val_loss: 6.7948 - 122ms/epoch - 2ms/step\n",
      "Epoch 442/800\n",
      "78/78 - 0s - loss: 6.8929 - val_loss: 6.8234 - 122ms/epoch - 2ms/step\n",
      "Epoch 443/800\n",
      "78/78 - 0s - loss: 6.8867 - val_loss: 6.7067 - 128ms/epoch - 2ms/step\n",
      "Epoch 444/800\n",
      "78/78 - 0s - loss: 6.9021 - val_loss: 6.7476 - 126ms/epoch - 2ms/step\n",
      "Epoch 445/800\n",
      "78/78 - 0s - loss: 6.8859 - val_loss: 6.7364 - 124ms/epoch - 2ms/step\n",
      "Epoch 446/800\n",
      "78/78 - 0s - loss: 6.9146 - val_loss: 6.7869 - 129ms/epoch - 2ms/step\n",
      "Epoch 447/800\n",
      "78/78 - 0s - loss: 6.9062 - val_loss: 6.7329 - 123ms/epoch - 2ms/step\n",
      "Epoch 448/800\n",
      "78/78 - 0s - loss: 6.9100 - val_loss: 6.7322 - 126ms/epoch - 2ms/step\n",
      "Epoch 449/800\n",
      "78/78 - 0s - loss: 6.8791 - val_loss: 6.7099 - 123ms/epoch - 2ms/step\n",
      "Epoch 450/800\n",
      "78/78 - 0s - loss: 6.8584 - val_loss: 6.7017 - 128ms/epoch - 2ms/step\n",
      "Epoch 451/800\n",
      "78/78 - 0s - loss: 6.8898 - val_loss: 6.7114 - 120ms/epoch - 2ms/step\n",
      "Epoch 452/800\n",
      "78/78 - 0s - loss: 6.8551 - val_loss: 6.7037 - 168ms/epoch - 2ms/step\n",
      "Epoch 453/800\n",
      "78/78 - 0s - loss: 6.8346 - val_loss: 6.7069 - 139ms/epoch - 2ms/step\n",
      "Epoch 454/800\n",
      "78/78 - 0s - loss: 6.8524 - val_loss: 6.7350 - 124ms/epoch - 2ms/step\n",
      "Epoch 455/800\n",
      "78/78 - 0s - loss: 6.8450 - val_loss: 6.6750 - 133ms/epoch - 2ms/step\n",
      "Epoch 456/800\n",
      "78/78 - 0s - loss: 6.8633 - val_loss: 6.7695 - 160ms/epoch - 2ms/step\n",
      "Epoch 457/800\n",
      "78/78 - 0s - loss: 6.9013 - val_loss: 6.7205 - 171ms/epoch - 2ms/step\n",
      "Epoch 458/800\n",
      "78/78 - 0s - loss: 6.8748 - val_loss: 6.7877 - 177ms/epoch - 2ms/step\n",
      "Epoch 459/800\n",
      "78/78 - 0s - loss: 6.8853 - val_loss: 6.7410 - 187ms/epoch - 2ms/step\n",
      "Epoch 460/800\n",
      "78/78 - 0s - loss: 6.8775 - val_loss: 6.7482 - 137ms/epoch - 2ms/step\n",
      "Epoch 461/800\n",
      "78/78 - 0s - loss: 6.8798 - val_loss: 6.7426 - 124ms/epoch - 2ms/step\n",
      "Epoch 462/800\n",
      "78/78 - 0s - loss: 6.8738 - val_loss: 6.7802 - 128ms/epoch - 2ms/step\n",
      "Epoch 463/800\n",
      "78/78 - 0s - loss: 6.8991 - val_loss: 6.7558 - 124ms/epoch - 2ms/step\n",
      "Epoch 464/800\n",
      "78/78 - 0s - loss: 6.9018 - val_loss: 6.8305 - 131ms/epoch - 2ms/step\n",
      "Epoch 465/800\n",
      "78/78 - 0s - loss: 6.9211 - val_loss: 6.7333 - 143ms/epoch - 2ms/step\n",
      "Epoch 466/800\n",
      "78/78 - 0s - loss: 6.9144 - val_loss: 6.7526 - 121ms/epoch - 2ms/step\n",
      "Epoch 467/800\n",
      "78/78 - 0s - loss: 6.8769 - val_loss: 6.7563 - 124ms/epoch - 2ms/step\n",
      "Epoch 468/800\n",
      "78/78 - 0s - loss: 6.8740 - val_loss: 6.6979 - 112ms/epoch - 1ms/step\n",
      "Epoch 469/800\n",
      "78/78 - 0s - loss: 6.8771 - val_loss: 6.7533 - 150ms/epoch - 2ms/step\n",
      "Epoch 470/800\n",
      "78/78 - 0s - loss: 6.8955 - val_loss: 6.7595 - 138ms/epoch - 2ms/step\n",
      "Epoch 471/800\n",
      "78/78 - 0s - loss: 6.8898 - val_loss: 6.7555 - 130ms/epoch - 2ms/step\n",
      "Epoch 472/800\n",
      "78/78 - 0s - loss: 6.8659 - val_loss: 6.7069 - 127ms/epoch - 2ms/step\n",
      "Epoch 473/800\n",
      "78/78 - 0s - loss: 6.8668 - val_loss: 6.6522 - 131ms/epoch - 2ms/step\n",
      "Epoch 474/800\n",
      "78/78 - 0s - loss: 6.8281 - val_loss: 6.6837 - 125ms/epoch - 2ms/step\n",
      "Epoch 475/800\n",
      "78/78 - 0s - loss: 6.8478 - val_loss: 6.7147 - 128ms/epoch - 2ms/step\n",
      "Epoch 476/800\n",
      "78/78 - 0s - loss: 6.8329 - val_loss: 6.6733 - 122ms/epoch - 2ms/step\n",
      "Epoch 477/800\n",
      "78/78 - 0s - loss: 6.8360 - val_loss: 6.6706 - 152ms/epoch - 2ms/step\n",
      "Epoch 478/800\n",
      "78/78 - 0s - loss: 6.8599 - val_loss: 6.7364 - 128ms/epoch - 2ms/step\n",
      "Epoch 479/800\n",
      "78/78 - 0s - loss: 6.8740 - val_loss: 6.7579 - 129ms/epoch - 2ms/step\n",
      "Epoch 480/800\n",
      "78/78 - 0s - loss: 6.8745 - val_loss: 6.7352 - 122ms/epoch - 2ms/step\n",
      "Epoch 481/800\n",
      "78/78 - 0s - loss: 6.8564 - val_loss: 6.6714 - 129ms/epoch - 2ms/step\n",
      "Epoch 482/800\n",
      "78/78 - 0s - loss: 6.8673 - val_loss: 6.7005 - 136ms/epoch - 2ms/step\n",
      "Epoch 483/800\n",
      "78/78 - 0s - loss: 6.8501 - val_loss: 6.7161 - 133ms/epoch - 2ms/step\n",
      "Epoch 484/800\n",
      "78/78 - 0s - loss: 6.8466 - val_loss: 6.7015 - 120ms/epoch - 2ms/step\n",
      "Epoch 485/800\n",
      "78/78 - 0s - loss: 6.8772 - val_loss: 6.6771 - 128ms/epoch - 2ms/step\n",
      "Epoch 486/800\n",
      "78/78 - 0s - loss: 6.8507 - val_loss: 6.7054 - 122ms/epoch - 2ms/step\n",
      "Epoch 487/800\n",
      "78/78 - 0s - loss: 6.8215 - val_loss: 6.6738 - 104ms/epoch - 1ms/step\n",
      "Epoch 488/800\n",
      "78/78 - 0s - loss: 6.8167 - val_loss: 6.6580 - 128ms/epoch - 2ms/step\n",
      "Epoch 489/800\n",
      "78/78 - 0s - loss: 6.8512 - val_loss: 6.7160 - 114ms/epoch - 1ms/step\n",
      "Epoch 490/800\n",
      "78/78 - 0s - loss: 6.8610 - val_loss: 6.7308 - 116ms/epoch - 1ms/step\n",
      "Epoch 491/800\n",
      "78/78 - 0s - loss: 6.8569 - val_loss: 6.7274 - 114ms/epoch - 1ms/step\n",
      "Epoch 492/800\n",
      "78/78 - 0s - loss: 6.9006 - val_loss: 6.7295 - 109ms/epoch - 1ms/step\n",
      "Epoch 493/800\n",
      "78/78 - 0s - loss: 6.8611 - val_loss: 6.7131 - 122ms/epoch - 2ms/step\n",
      "Epoch 494/800\n",
      "78/78 - 0s - loss: 6.8380 - val_loss: 6.6378 - 117ms/epoch - 2ms/step\n",
      "Epoch 495/800\n",
      "78/78 - 0s - loss: 6.8286 - val_loss: 6.6604 - 121ms/epoch - 2ms/step\n",
      "Epoch 496/800\n",
      "78/78 - 0s - loss: 6.8243 - val_loss: 6.7219 - 120ms/epoch - 2ms/step\n",
      "Epoch 497/800\n",
      "78/78 - 0s - loss: 6.8430 - val_loss: 6.6625 - 113ms/epoch - 1ms/step\n",
      "Epoch 498/800\n",
      "78/78 - 0s - loss: 6.8453 - val_loss: 6.6840 - 116ms/epoch - 1ms/step\n",
      "Epoch 499/800\n",
      "78/78 - 0s - loss: 6.8212 - val_loss: 6.7242 - 117ms/epoch - 2ms/step\n",
      "Epoch 500/800\n",
      "78/78 - 0s - loss: 6.8622 - val_loss: 6.6762 - 115ms/epoch - 1ms/step\n",
      "Epoch 501/800\n",
      "78/78 - 0s - loss: 6.8395 - val_loss: 6.6798 - 102ms/epoch - 1ms/step\n",
      "Epoch 502/800\n",
      "78/78 - 0s - loss: 6.8305 - val_loss: 6.7008 - 115ms/epoch - 1ms/step\n",
      "Epoch 503/800\n",
      "78/78 - 0s - loss: 6.8393 - val_loss: 6.6555 - 125ms/epoch - 2ms/step\n",
      "Epoch 504/800\n",
      "78/78 - 0s - loss: 6.8547 - val_loss: 6.7058 - 116ms/epoch - 1ms/step\n",
      "Epoch 505/800\n",
      "78/78 - 0s - loss: 6.8493 - val_loss: 6.6753 - 114ms/epoch - 1ms/step\n",
      "Epoch 506/800\n",
      "78/78 - 0s - loss: 6.8283 - val_loss: 6.6752 - 109ms/epoch - 1ms/step\n",
      "Epoch 507/800\n",
      "78/78 - 0s - loss: 6.8119 - val_loss: 6.7026 - 116ms/epoch - 1ms/step\n",
      "Epoch 508/800\n",
      "78/78 - 0s - loss: 6.8034 - val_loss: 6.6503 - 100ms/epoch - 1ms/step\n",
      "Epoch 509/800\n",
      "78/78 - 0s - loss: 6.8165 - val_loss: 6.6483 - 116ms/epoch - 1ms/step\n",
      "Epoch 510/800\n",
      "78/78 - 0s - loss: 6.7988 - val_loss: 6.6689 - 116ms/epoch - 1ms/step\n",
      "Epoch 511/800\n",
      "78/78 - 0s - loss: 6.8340 - val_loss: 6.6979 - 103ms/epoch - 1ms/step\n",
      "Epoch 512/800\n",
      "78/78 - 0s - loss: 6.8032 - val_loss: 6.6923 - 116ms/epoch - 1ms/step\n",
      "Epoch 513/800\n",
      "78/78 - 0s - loss: 6.8276 - val_loss: 6.6683 - 129ms/epoch - 2ms/step\n",
      "Epoch 514/800\n",
      "78/78 - 0s - loss: 6.8156 - val_loss: 6.6390 - 102ms/epoch - 1ms/step\n",
      "Epoch 515/800\n",
      "78/78 - 0s - loss: 6.8516 - val_loss: 6.6955 - 115ms/epoch - 1ms/step\n",
      "Epoch 516/800\n",
      "78/78 - 0s - loss: 6.8270 - val_loss: 6.6743 - 127ms/epoch - 2ms/step\n",
      "Epoch 517/800\n",
      "78/78 - 0s - loss: 6.8140 - val_loss: 6.7137 - 108ms/epoch - 1ms/step\n",
      "Epoch 518/800\n",
      "78/78 - 0s - loss: 6.8315 - val_loss: 6.6470 - 123ms/epoch - 2ms/step\n",
      "Epoch 519/800\n",
      "78/78 - 0s - loss: 6.7992 - val_loss: 6.6199 - 108ms/epoch - 1ms/step\n",
      "Epoch 520/800\n",
      "78/78 - 0s - loss: 6.7926 - val_loss: 6.6588 - 116ms/epoch - 1ms/step\n",
      "Epoch 521/800\n",
      "78/78 - 0s - loss: 6.7905 - val_loss: 6.6120 - 117ms/epoch - 1ms/step\n",
      "Epoch 522/800\n",
      "78/78 - 0s - loss: 6.8133 - val_loss: 6.7250 - 115ms/epoch - 1ms/step\n",
      "Epoch 523/800\n",
      "78/78 - 0s - loss: 6.8713 - val_loss: 6.6737 - 103ms/epoch - 1ms/step\n",
      "Epoch 524/800\n",
      "78/78 - 0s - loss: 6.8256 - val_loss: 6.7644 - 119ms/epoch - 2ms/step\n",
      "Epoch 525/800\n",
      "78/78 - 0s - loss: 6.8754 - val_loss: 6.6752 - 115ms/epoch - 1ms/step\n",
      "Epoch 526/800\n",
      "78/78 - 0s - loss: 6.8287 - val_loss: 6.6606 - 126ms/epoch - 2ms/step\n",
      "Epoch 527/800\n",
      "78/78 - 0s - loss: 6.8298 - val_loss: 6.6269 - 107ms/epoch - 1ms/step\n",
      "Epoch 528/800\n",
      "78/78 - 0s - loss: 6.8484 - val_loss: 6.6771 - 125ms/epoch - 2ms/step\n",
      "Epoch 529/800\n",
      "78/78 - 0s - loss: 6.8327 - val_loss: 6.6751 - 113ms/epoch - 1ms/step\n",
      "Epoch 530/800\n",
      "78/78 - 0s - loss: 6.8128 - val_loss: 6.6313 - 111ms/epoch - 1ms/step\n",
      "Epoch 531/800\n",
      "78/78 - 0s - loss: 6.8028 - val_loss: 6.7004 - 116ms/epoch - 1ms/step\n",
      "Epoch 532/800\n",
      "78/78 - 0s - loss: 6.8134 - val_loss: 6.6710 - 117ms/epoch - 1ms/step\n",
      "Epoch 533/800\n",
      "78/78 - 0s - loss: 6.8402 - val_loss: 6.6599 - 102ms/epoch - 1ms/step\n",
      "Epoch 534/800\n",
      "78/78 - 0s - loss: 6.8412 - val_loss: 6.7191 - 116ms/epoch - 1ms/step\n",
      "Epoch 535/800\n",
      "78/78 - 0s - loss: 6.8549 - val_loss: 6.6703 - 100ms/epoch - 1ms/step\n",
      "Epoch 536/800\n",
      "78/78 - 0s - loss: 6.8085 - val_loss: 6.6625 - 130ms/epoch - 2ms/step\n",
      "Epoch 537/800\n",
      "78/78 - 0s - loss: 6.7702 - val_loss: 6.6466 - 101ms/epoch - 1ms/step\n",
      "Epoch 538/800\n",
      "78/78 - 0s - loss: 6.7858 - val_loss: 6.6580 - 121ms/epoch - 2ms/step\n",
      "Epoch 539/800\n",
      "78/78 - 0s - loss: 6.8175 - val_loss: 6.6771 - 111ms/epoch - 1ms/step\n",
      "Epoch 540/800\n",
      "78/78 - 0s - loss: 6.8186 - val_loss: 6.7144 - 125ms/epoch - 2ms/step\n",
      "Epoch 541/800\n",
      "78/78 - 0s - loss: 6.8287 - val_loss: 6.6363 - 126ms/epoch - 2ms/step\n",
      "Epoch 542/800\n",
      "78/78 - 0s - loss: 6.7923 - val_loss: 6.6715 - 112ms/epoch - 1ms/step\n",
      "Epoch 543/800\n",
      "78/78 - 0s - loss: 6.7904 - val_loss: 6.6189 - 104ms/epoch - 1ms/step\n",
      "Epoch 544/800\n",
      "78/78 - 0s - loss: 6.7985 - val_loss: 6.6614 - 117ms/epoch - 1ms/step\n",
      "Epoch 545/800\n",
      "78/78 - 0s - loss: 6.7884 - val_loss: 6.6193 - 123ms/epoch - 2ms/step\n",
      "Epoch 546/800\n",
      "78/78 - 0s - loss: 6.7910 - val_loss: 6.7306 - 109ms/epoch - 1ms/step\n",
      "Epoch 547/800\n",
      "78/78 - 0s - loss: 6.8073 - val_loss: 6.6302 - 118ms/epoch - 2ms/step\n",
      "Epoch 548/800\n",
      "78/78 - 0s - loss: 6.7784 - val_loss: 6.6426 - 117ms/epoch - 1ms/step\n",
      "Epoch 549/800\n",
      "78/78 - 0s - loss: 6.7877 - val_loss: 6.6056 - 118ms/epoch - 2ms/step\n",
      "Epoch 550/800\n",
      "78/78 - 0s - loss: 6.7667 - val_loss: 6.6091 - 100ms/epoch - 1ms/step\n",
      "Epoch 551/800\n",
      "78/78 - 0s - loss: 6.7752 - val_loss: 6.6453 - 116ms/epoch - 1ms/step\n",
      "Epoch 552/800\n",
      "78/78 - 0s - loss: 6.7756 - val_loss: 6.6460 - 116ms/epoch - 1ms/step\n",
      "Epoch 553/800\n",
      "78/78 - 0s - loss: 6.7927 - val_loss: 6.6511 - 145ms/epoch - 2ms/step\n",
      "Epoch 554/800\n",
      "78/78 - 0s - loss: 6.8081 - val_loss: 6.7027 - 120ms/epoch - 2ms/step\n",
      "Epoch 555/800\n",
      "78/78 - 0s - loss: 6.7965 - val_loss: 6.5958 - 115ms/epoch - 1ms/step\n",
      "Epoch 556/800\n",
      "78/78 - 0s - loss: 6.7778 - val_loss: 6.5990 - 100ms/epoch - 1ms/step\n",
      "Epoch 557/800\n",
      "78/78 - 0s - loss: 6.7940 - val_loss: 6.6429 - 116ms/epoch - 1ms/step\n",
      "Epoch 558/800\n",
      "78/78 - 0s - loss: 6.7748 - val_loss: 6.6167 - 119ms/epoch - 2ms/step\n",
      "Epoch 559/800\n",
      "78/78 - 0s - loss: 6.7981 - val_loss: 6.6400 - 132ms/epoch - 2ms/step\n",
      "Epoch 560/800\n",
      "78/78 - 0s - loss: 6.7722 - val_loss: 6.6006 - 151ms/epoch - 2ms/step\n",
      "Epoch 561/800\n",
      "78/78 - 0s - loss: 6.7791 - val_loss: 6.6243 - 114ms/epoch - 1ms/step\n",
      "Epoch 562/800\n",
      "78/78 - 0s - loss: 6.7969 - val_loss: 6.6536 - 117ms/epoch - 1ms/step\n",
      "Epoch 563/800\n",
      "78/78 - 0s - loss: 6.8079 - val_loss: 6.7454 - 123ms/epoch - 2ms/step\n",
      "Epoch 564/800\n",
      "78/78 - 0s - loss: 6.8288 - val_loss: 6.6995 - 128ms/epoch - 2ms/step\n",
      "Epoch 565/800\n",
      "78/78 - 0s - loss: 6.8518 - val_loss: 6.7117 - 139ms/epoch - 2ms/step\n",
      "Epoch 566/800\n",
      "78/78 - 0s - loss: 6.7949 - val_loss: 6.6136 - 125ms/epoch - 2ms/step\n",
      "Epoch 567/800\n",
      "78/78 - 0s - loss: 6.7804 - val_loss: 6.6368 - 122ms/epoch - 2ms/step\n",
      "Epoch 568/800\n",
      "78/78 - 0s - loss: 6.7905 - val_loss: 6.6143 - 122ms/epoch - 2ms/step\n",
      "Epoch 569/800\n",
      "78/78 - 0s - loss: 6.7763 - val_loss: 6.6466 - 126ms/epoch - 2ms/step\n",
      "Epoch 570/800\n",
      "78/78 - 0s - loss: 6.7720 - val_loss: 6.6415 - 120ms/epoch - 2ms/step\n",
      "Epoch 571/800\n",
      "78/78 - 0s - loss: 6.7874 - val_loss: 6.6547 - 118ms/epoch - 2ms/step\n",
      "Epoch 572/800\n",
      "78/78 - 0s - loss: 6.7823 - val_loss: 6.6499 - 116ms/epoch - 1ms/step\n",
      "Epoch 573/800\n",
      "78/78 - 0s - loss: 6.8180 - val_loss: 6.6198 - 116ms/epoch - 1ms/step\n",
      "Epoch 574/800\n",
      "78/78 - 0s - loss: 6.8048 - val_loss: 6.6176 - 118ms/epoch - 2ms/step\n",
      "Epoch 575/800\n",
      "78/78 - 0s - loss: 6.7715 - val_loss: 6.6056 - 99ms/epoch - 1ms/step\n",
      "Epoch 576/800\n",
      "78/78 - 0s - loss: 6.7749 - val_loss: 6.6084 - 132ms/epoch - 2ms/step\n",
      "Epoch 577/800\n",
      "78/78 - 0s - loss: 6.7766 - val_loss: 6.6457 - 116ms/epoch - 1ms/step\n",
      "Epoch 578/800\n",
      "78/78 - 0s - loss: 6.7747 - val_loss: 6.6361 - 99ms/epoch - 1ms/step\n",
      "Epoch 579/800\n",
      "78/78 - 0s - loss: 6.7879 - val_loss: 6.6437 - 101ms/epoch - 1ms/step\n",
      "Epoch 580/800\n",
      "78/78 - 0s - loss: 6.7492 - val_loss: 6.5881 - 119ms/epoch - 2ms/step\n",
      "Epoch 581/800\n",
      "78/78 - 0s - loss: 6.7536 - val_loss: 6.6253 - 116ms/epoch - 1ms/step\n",
      "Epoch 582/800\n",
      "78/78 - 0s - loss: 6.7746 - val_loss: 6.7046 - 130ms/epoch - 2ms/step\n",
      "Epoch 583/800\n",
      "78/78 - 0s - loss: 6.7654 - val_loss: 6.6321 - 117ms/epoch - 2ms/step\n",
      "Epoch 584/800\n",
      "78/78 - 0s - loss: 6.7621 - val_loss: 6.6473 - 149ms/epoch - 2ms/step\n",
      "Epoch 585/800\n",
      "78/78 - 0s - loss: 6.7931 - val_loss: 6.6364 - 125ms/epoch - 2ms/step\n",
      "Epoch 586/800\n",
      "78/78 - 0s - loss: 6.7836 - val_loss: 6.6675 - 130ms/epoch - 2ms/step\n",
      "Epoch 587/800\n",
      "78/78 - 0s - loss: 6.7936 - val_loss: 6.5867 - 128ms/epoch - 2ms/step\n",
      "Epoch 588/800\n",
      "78/78 - 0s - loss: 6.7656 - val_loss: 6.6181 - 127ms/epoch - 2ms/step\n",
      "Epoch 589/800\n",
      "78/78 - 0s - loss: 6.7702 - val_loss: 6.6368 - 127ms/epoch - 2ms/step\n",
      "Epoch 590/800\n",
      "78/78 - 0s - loss: 6.7624 - val_loss: 6.6000 - 125ms/epoch - 2ms/step\n",
      "Epoch 591/800\n",
      "78/78 - 0s - loss: 6.7443 - val_loss: 6.6063 - 128ms/epoch - 2ms/step\n",
      "Epoch 592/800\n",
      "78/78 - 0s - loss: 6.7785 - val_loss: 6.6355 - 127ms/epoch - 2ms/step\n",
      "Epoch 593/800\n",
      "78/78 - 0s - loss: 6.7895 - val_loss: 6.6521 - 123ms/epoch - 2ms/step\n",
      "Epoch 594/800\n",
      "78/78 - 0s - loss: 6.7675 - val_loss: 6.6225 - 127ms/epoch - 2ms/step\n",
      "Epoch 595/800\n",
      "78/78 - 0s - loss: 6.7602 - val_loss: 6.5774 - 123ms/epoch - 2ms/step\n",
      "Epoch 596/800\n",
      "78/78 - 0s - loss: 6.7409 - val_loss: 6.5870 - 126ms/epoch - 2ms/step\n",
      "Epoch 597/800\n",
      "78/78 - 0s - loss: 6.7459 - val_loss: 6.6120 - 128ms/epoch - 2ms/step\n",
      "Epoch 598/800\n",
      "78/78 - 0s - loss: 6.7921 - val_loss: 6.6010 - 123ms/epoch - 2ms/step\n",
      "Epoch 599/800\n",
      "78/78 - 0s - loss: 6.7990 - val_loss: 6.6208 - 131ms/epoch - 2ms/step\n",
      "Epoch 600/800\n",
      "78/78 - 0s - loss: 6.7876 - val_loss: 6.6676 - 121ms/epoch - 2ms/step\n",
      "Epoch 601/800\n",
      "78/78 - 0s - loss: 6.7935 - val_loss: 6.6302 - 118ms/epoch - 2ms/step\n",
      "Epoch 602/800\n",
      "78/78 - 0s - loss: 6.7746 - val_loss: 6.6290 - 123ms/epoch - 2ms/step\n",
      "Epoch 603/800\n",
      "78/78 - 0s - loss: 6.7634 - val_loss: 6.6228 - 125ms/epoch - 2ms/step\n",
      "Epoch 604/800\n",
      "78/78 - 0s - loss: 6.7756 - val_loss: 6.6009 - 125ms/epoch - 2ms/step\n",
      "Epoch 605/800\n",
      "78/78 - 0s - loss: 6.7371 - val_loss: 6.6120 - 122ms/epoch - 2ms/step\n",
      "Epoch 606/800\n",
      "78/78 - 0s - loss: 6.7445 - val_loss: 6.6157 - 121ms/epoch - 2ms/step\n",
      "Epoch 607/800\n",
      "78/78 - 0s - loss: 6.7630 - val_loss: 6.6178 - 128ms/epoch - 2ms/step\n",
      "Epoch 608/800\n",
      "78/78 - 0s - loss: 6.7831 - val_loss: 6.6709 - 127ms/epoch - 2ms/step\n",
      "Epoch 609/800\n",
      "78/78 - 0s - loss: 6.7759 - val_loss: 6.6429 - 120ms/epoch - 2ms/step\n",
      "Epoch 610/800\n",
      "78/78 - 0s - loss: 6.7549 - val_loss: 6.5985 - 125ms/epoch - 2ms/step\n",
      "Epoch 611/800\n",
      "78/78 - 0s - loss: 6.7459 - val_loss: 6.5665 - 120ms/epoch - 2ms/step\n",
      "Epoch 612/800\n",
      "78/78 - 0s - loss: 6.7148 - val_loss: 6.5746 - 121ms/epoch - 2ms/step\n",
      "Epoch 613/800\n",
      "78/78 - 0s - loss: 6.7139 - val_loss: 6.6482 - 127ms/epoch - 2ms/step\n",
      "Epoch 614/800\n",
      "78/78 - 0s - loss: 6.7429 - val_loss: 6.5796 - 129ms/epoch - 2ms/step\n",
      "Epoch 615/800\n",
      "78/78 - 0s - loss: 6.7674 - val_loss: 6.6261 - 148ms/epoch - 2ms/step\n",
      "Epoch 616/800\n",
      "78/78 - 0s - loss: 6.7591 - val_loss: 6.6439 - 163ms/epoch - 2ms/step\n",
      "Epoch 617/800\n",
      "78/78 - 0s - loss: 6.7786 - val_loss: 6.5979 - 136ms/epoch - 2ms/step\n",
      "Epoch 618/800\n",
      "78/78 - 0s - loss: 6.7758 - val_loss: 6.6085 - 133ms/epoch - 2ms/step\n",
      "Epoch 619/800\n",
      "78/78 - 0s - loss: 6.7633 - val_loss: 6.5909 - 125ms/epoch - 2ms/step\n",
      "Epoch 620/800\n",
      "78/78 - 0s - loss: 6.7107 - val_loss: 6.6100 - 130ms/epoch - 2ms/step\n",
      "Epoch 621/800\n",
      "78/78 - 0s - loss: 6.7189 - val_loss: 6.5772 - 129ms/epoch - 2ms/step\n",
      "Epoch 622/800\n",
      "78/78 - 0s - loss: 6.7264 - val_loss: 6.5978 - 130ms/epoch - 2ms/step\n",
      "Epoch 623/800\n",
      "78/78 - 0s - loss: 6.7550 - val_loss: 6.6163 - 134ms/epoch - 2ms/step\n",
      "Epoch 624/800\n",
      "78/78 - 0s - loss: 6.7418 - val_loss: 6.5640 - 118ms/epoch - 2ms/step\n",
      "Epoch 625/800\n",
      "78/78 - 0s - loss: 6.8354 - val_loss: 6.6791 - 124ms/epoch - 2ms/step\n",
      "Epoch 626/800\n",
      "78/78 - 0s - loss: 6.7979 - val_loss: 6.6067 - 120ms/epoch - 2ms/step\n",
      "Epoch 627/800\n",
      "78/78 - 0s - loss: 6.7427 - val_loss: 6.5806 - 133ms/epoch - 2ms/step\n",
      "Epoch 628/800\n",
      "78/78 - 0s - loss: 6.7186 - val_loss: 6.5554 - 170ms/epoch - 2ms/step\n",
      "Epoch 629/800\n",
      "78/78 - 0s - loss: 6.6981 - val_loss: 6.5704 - 152ms/epoch - 2ms/step\n",
      "Epoch 630/800\n",
      "78/78 - 0s - loss: 6.7177 - val_loss: 6.5831 - 120ms/epoch - 2ms/step\n",
      "Epoch 631/800\n",
      "78/78 - 0s - loss: 6.7519 - val_loss: 6.6556 - 129ms/epoch - 2ms/step\n",
      "Epoch 632/800\n",
      "78/78 - 0s - loss: 6.7597 - val_loss: 6.5970 - 124ms/epoch - 2ms/step\n",
      "Epoch 633/800\n",
      "78/78 - 0s - loss: 6.7603 - val_loss: 6.5914 - 121ms/epoch - 2ms/step\n",
      "Epoch 634/800\n",
      "78/78 - 0s - loss: 6.7469 - val_loss: 6.6428 - 118ms/epoch - 2ms/step\n",
      "Epoch 635/800\n",
      "78/78 - 0s - loss: 6.7683 - val_loss: 6.6356 - 121ms/epoch - 2ms/step\n",
      "Epoch 636/800\n",
      "78/78 - 0s - loss: 6.7645 - val_loss: 6.6316 - 120ms/epoch - 2ms/step\n",
      "Epoch 637/800\n",
      "78/78 - 0s - loss: 6.7513 - val_loss: 6.5745 - 116ms/epoch - 1ms/step\n",
      "Epoch 638/800\n",
      "78/78 - 0s - loss: 6.7129 - val_loss: 6.5256 - 118ms/epoch - 2ms/step\n",
      "Epoch 639/800\n",
      "78/78 - 0s - loss: 6.7048 - val_loss: 6.5670 - 118ms/epoch - 2ms/step\n",
      "Epoch 640/800\n",
      "78/78 - 0s - loss: 6.7447 - val_loss: 6.5627 - 137ms/epoch - 2ms/step\n",
      "Epoch 641/800\n",
      "78/78 - 0s - loss: 6.7366 - val_loss: 6.5647 - 130ms/epoch - 2ms/step\n",
      "Epoch 642/800\n",
      "78/78 - 0s - loss: 6.7447 - val_loss: 6.6065 - 125ms/epoch - 2ms/step\n",
      "Epoch 643/800\n",
      "78/78 - 0s - loss: 6.7474 - val_loss: 6.6466 - 122ms/epoch - 2ms/step\n",
      "Epoch 644/800\n",
      "78/78 - 0s - loss: 6.7430 - val_loss: 6.5725 - 123ms/epoch - 2ms/step\n",
      "Epoch 645/800\n",
      "78/78 - 0s - loss: 6.7484 - val_loss: 6.5854 - 120ms/epoch - 2ms/step\n",
      "Epoch 646/800\n",
      "78/78 - 0s - loss: 6.7416 - val_loss: 6.5691 - 123ms/epoch - 2ms/step\n",
      "Epoch 647/800\n",
      "78/78 - 0s - loss: 6.7374 - val_loss: 6.6421 - 126ms/epoch - 2ms/step\n",
      "Epoch 648/800\n",
      "78/78 - 0s - loss: 6.7261 - val_loss: 6.6354 - 121ms/epoch - 2ms/step\n",
      "Epoch 649/800\n",
      "78/78 - 0s - loss: 6.7087 - val_loss: 6.5152 - 119ms/epoch - 2ms/step\n",
      "Epoch 650/800\n",
      "78/78 - 0s - loss: 6.6838 - val_loss: 6.5238 - 122ms/epoch - 2ms/step\n",
      "Epoch 651/800\n",
      "78/78 - 0s - loss: 6.7258 - val_loss: 6.5581 - 122ms/epoch - 2ms/step\n",
      "Epoch 652/800\n",
      "78/78 - 0s - loss: 6.7430 - val_loss: 6.5954 - 119ms/epoch - 2ms/step\n",
      "Epoch 653/800\n",
      "78/78 - 0s - loss: 6.7671 - val_loss: 6.6512 - 120ms/epoch - 2ms/step\n",
      "Epoch 654/800\n",
      "78/78 - 0s - loss: 6.7614 - val_loss: 6.5940 - 118ms/epoch - 2ms/step\n",
      "Epoch 655/800\n",
      "78/78 - 0s - loss: 6.7235 - val_loss: 6.5498 - 123ms/epoch - 2ms/step\n",
      "Epoch 656/800\n",
      "78/78 - 0s - loss: 6.7273 - val_loss: 6.6234 - 119ms/epoch - 2ms/step\n",
      "Epoch 657/800\n",
      "78/78 - 0s - loss: 6.7419 - val_loss: 6.5738 - 119ms/epoch - 2ms/step\n",
      "Epoch 658/800\n",
      "78/78 - 0s - loss: 6.7268 - val_loss: 6.5672 - 118ms/epoch - 2ms/step\n",
      "Epoch 659/800\n",
      "78/78 - 0s - loss: 6.6991 - val_loss: 6.5638 - 121ms/epoch - 2ms/step\n",
      "Epoch 660/800\n",
      "78/78 - 0s - loss: 6.7019 - val_loss: 6.5325 - 121ms/epoch - 2ms/step\n",
      "Epoch 661/800\n",
      "78/78 - 0s - loss: 6.7178 - val_loss: 6.5973 - 123ms/epoch - 2ms/step\n",
      "Epoch 662/800\n",
      "78/78 - 0s - loss: 6.7052 - val_loss: 6.5640 - 116ms/epoch - 1ms/step\n",
      "Epoch 663/800\n",
      "78/78 - 0s - loss: 6.7084 - val_loss: 6.5337 - 123ms/epoch - 2ms/step\n",
      "Epoch 664/800\n",
      "78/78 - 0s - loss: 6.6993 - val_loss: 6.5772 - 125ms/epoch - 2ms/step\n",
      "Epoch 665/800\n",
      "78/78 - 0s - loss: 6.7154 - val_loss: 6.5655 - 118ms/epoch - 2ms/step\n",
      "Epoch 666/800\n",
      "78/78 - 0s - loss: 6.7203 - val_loss: 6.6213 - 120ms/epoch - 2ms/step\n",
      "Epoch 667/800\n",
      "78/78 - 0s - loss: 6.7419 - val_loss: 6.5645 - 118ms/epoch - 2ms/step\n",
      "Epoch 668/800\n",
      "78/78 - 0s - loss: 6.7161 - val_loss: 6.5528 - 116ms/epoch - 1ms/step\n",
      "Epoch 669/800\n",
      "78/78 - 0s - loss: 6.7100 - val_loss: 6.5664 - 116ms/epoch - 1ms/step\n",
      "Epoch 670/800\n",
      "78/78 - 0s - loss: 6.6970 - val_loss: 6.5531 - 120ms/epoch - 2ms/step\n",
      "Epoch 671/800\n",
      "78/78 - 0s - loss: 6.7379 - val_loss: 6.5759 - 118ms/epoch - 2ms/step\n",
      "Epoch 672/800\n",
      "78/78 - 0s - loss: 6.7343 - val_loss: 6.5377 - 118ms/epoch - 2ms/step\n",
      "Epoch 673/800\n",
      "78/78 - 0s - loss: 6.6906 - val_loss: 6.5649 - 117ms/epoch - 1ms/step\n",
      "Epoch 674/800\n",
      "78/78 - 0s - loss: 6.6917 - val_loss: 6.5583 - 117ms/epoch - 1ms/step\n",
      "Epoch 675/800\n",
      "78/78 - 0s - loss: 6.6888 - val_loss: 6.5753 - 119ms/epoch - 2ms/step\n",
      "Epoch 676/800\n",
      "78/78 - 0s - loss: 6.7192 - val_loss: 6.5430 - 116ms/epoch - 1ms/step\n",
      "Epoch 677/800\n",
      "78/78 - 0s - loss: 6.7165 - val_loss: 6.6336 - 121ms/epoch - 2ms/step\n",
      "Epoch 678/800\n",
      "78/78 - 0s - loss: 6.7748 - val_loss: 6.5761 - 148ms/epoch - 2ms/step\n",
      "Epoch 679/800\n",
      "78/78 - 0s - loss: 6.7352 - val_loss: 6.6147 - 131ms/epoch - 2ms/step\n",
      "Epoch 680/800\n",
      "78/78 - 0s - loss: 6.7710 - val_loss: 6.6006 - 146ms/epoch - 2ms/step\n",
      "Epoch 681/800\n",
      "78/78 - 0s - loss: 6.7373 - val_loss: 6.5867 - 154ms/epoch - 2ms/step\n",
      "Epoch 682/800\n",
      "78/78 - 0s - loss: 6.7067 - val_loss: 6.5481 - 158ms/epoch - 2ms/step\n",
      "Epoch 683/800\n",
      "78/78 - 0s - loss: 6.6831 - val_loss: 6.6067 - 146ms/epoch - 2ms/step\n",
      "Epoch 684/800\n",
      "78/78 - 0s - loss: 6.6863 - val_loss: 6.5501 - 155ms/epoch - 2ms/step\n",
      "Epoch 685/800\n",
      "78/78 - 0s - loss: 6.6936 - val_loss: 6.5539 - 145ms/epoch - 2ms/step\n",
      "Epoch 686/800\n",
      "78/78 - 0s - loss: 6.7085 - val_loss: 6.5435 - 139ms/epoch - 2ms/step\n",
      "Epoch 687/800\n",
      "78/78 - 0s - loss: 6.7137 - val_loss: 6.5371 - 141ms/epoch - 2ms/step\n",
      "Epoch 688/800\n",
      "78/78 - 0s - loss: 6.7023 - val_loss: 6.5434 - 138ms/epoch - 2ms/step\n",
      "Epoch 689/800\n",
      "78/78 - 0s - loss: 6.7155 - val_loss: 6.5784 - 138ms/epoch - 2ms/step\n",
      "Epoch 690/800\n",
      "78/78 - 0s - loss: 6.7221 - val_loss: 6.5452 - 161ms/epoch - 2ms/step\n",
      "Epoch 691/800\n",
      "78/78 - 0s - loss: 6.7335 - val_loss: 6.5721 - 161ms/epoch - 2ms/step\n",
      "Epoch 692/800\n",
      "78/78 - 0s - loss: 6.7294 - val_loss: 6.5693 - 145ms/epoch - 2ms/step\n",
      "Epoch 693/800\n",
      "78/78 - 0s - loss: 6.6991 - val_loss: 6.5413 - 117ms/epoch - 2ms/step\n",
      "Epoch 694/800\n",
      "78/78 - 0s - loss: 6.7128 - val_loss: 6.5589 - 171ms/epoch - 2ms/step\n",
      "Epoch 695/800\n",
      "78/78 - 0s - loss: 6.7120 - val_loss: 6.5480 - 157ms/epoch - 2ms/step\n",
      "Epoch 696/800\n",
      "78/78 - 0s - loss: 6.6770 - val_loss: 6.5161 - 153ms/epoch - 2ms/step\n",
      "Epoch 697/800\n",
      "78/78 - 0s - loss: 6.6741 - val_loss: 6.5084 - 128ms/epoch - 2ms/step\n",
      "Epoch 698/800\n",
      "78/78 - 0s - loss: 6.7127 - val_loss: 6.5717 - 118ms/epoch - 2ms/step\n",
      "Epoch 699/800\n",
      "78/78 - 0s - loss: 6.7078 - val_loss: 6.5248 - 120ms/epoch - 2ms/step\n",
      "Epoch 700/800\n",
      "78/78 - 0s - loss: 6.7126 - val_loss: 6.5860 - 120ms/epoch - 2ms/step\n",
      "Epoch 701/800\n",
      "78/78 - 0s - loss: 6.7051 - val_loss: 6.5685 - 117ms/epoch - 1ms/step\n",
      "Epoch 702/800\n",
      "78/78 - 0s - loss: 6.7358 - val_loss: 6.5791 - 119ms/epoch - 2ms/step\n",
      "Epoch 703/800\n",
      "78/78 - 0s - loss: 6.6979 - val_loss: 6.5858 - 117ms/epoch - 2ms/step\n",
      "Epoch 704/800\n",
      "78/78 - 0s - loss: 6.7052 - val_loss: 6.5526 - 117ms/epoch - 2ms/step\n",
      "Epoch 705/800\n",
      "78/78 - 0s - loss: 6.6874 - val_loss: 6.5511 - 117ms/epoch - 2ms/step\n",
      "Epoch 706/800\n",
      "78/78 - 0s - loss: 6.6955 - val_loss: 6.5637 - 116ms/epoch - 1ms/step\n",
      "Epoch 707/800\n",
      "78/78 - 0s - loss: 6.7144 - val_loss: 6.5468 - 122ms/epoch - 2ms/step\n",
      "Epoch 708/800\n",
      "78/78 - 0s - loss: 6.7112 - val_loss: 6.5513 - 120ms/epoch - 2ms/step\n",
      "Epoch 709/800\n",
      "78/78 - 0s - loss: 6.7108 - val_loss: 6.5834 - 128ms/epoch - 2ms/step\n",
      "Epoch 710/800\n",
      "78/78 - 0s - loss: 6.6871 - val_loss: 6.5559 - 185ms/epoch - 2ms/step\n",
      "Epoch 711/800\n",
      "78/78 - 0s - loss: 6.7369 - val_loss: 6.5846 - 119ms/epoch - 2ms/step\n",
      "Epoch 712/800\n",
      "78/78 - 0s - loss: 6.7263 - val_loss: 6.6821 - 115ms/epoch - 1ms/step\n",
      "Epoch 713/800\n",
      "78/78 - 0s - loss: 6.7086 - val_loss: 6.5488 - 117ms/epoch - 1ms/step\n",
      "Epoch 714/800\n",
      "78/78 - 0s - loss: 6.6934 - val_loss: 6.5448 - 124ms/epoch - 2ms/step\n",
      "Epoch 715/800\n",
      "78/78 - 0s - loss: 6.6706 - val_loss: 6.5282 - 163ms/epoch - 2ms/step\n",
      "Epoch 716/800\n",
      "78/78 - 0s - loss: 6.6737 - val_loss: 6.5806 - 122ms/epoch - 2ms/step\n",
      "Epoch 717/800\n",
      "78/78 - 0s - loss: 6.6731 - val_loss: 6.6266 - 134ms/epoch - 2ms/step\n",
      "Epoch 718/800\n",
      "78/78 - 0s - loss: 6.7281 - val_loss: 6.5708 - 138ms/epoch - 2ms/step\n",
      "Epoch 719/800\n",
      "78/78 - 0s - loss: 6.7398 - val_loss: 6.6508 - 137ms/epoch - 2ms/step\n",
      "Epoch 720/800\n",
      "78/78 - 0s - loss: 6.7434 - val_loss: 6.6381 - 137ms/epoch - 2ms/step\n",
      "Epoch 721/800\n",
      "78/78 - 0s - loss: 6.7461 - val_loss: 6.5959 - 146ms/epoch - 2ms/step\n",
      "Epoch 722/800\n",
      "78/78 - 0s - loss: 6.7320 - val_loss: 6.5937 - 138ms/epoch - 2ms/step\n",
      "Epoch 723/800\n",
      "78/78 - 0s - loss: 6.7144 - val_loss: 6.5227 - 211ms/epoch - 3ms/step\n",
      "Epoch 724/800\n",
      "78/78 - 0s - loss: 6.6867 - val_loss: 6.5320 - 141ms/epoch - 2ms/step\n",
      "Epoch 725/800\n",
      "78/78 - 0s - loss: 6.6900 - val_loss: 6.5431 - 174ms/epoch - 2ms/step\n",
      "Epoch 726/800\n",
      "78/78 - 0s - loss: 6.6719 - val_loss: 6.5200 - 167ms/epoch - 2ms/step\n",
      "Epoch 727/800\n",
      "78/78 - 0s - loss: 6.6643 - val_loss: 6.5105 - 166ms/epoch - 2ms/step\n",
      "Epoch 728/800\n",
      "78/78 - 0s - loss: 6.6754 - val_loss: 6.5397 - 138ms/epoch - 2ms/step\n",
      "Epoch 729/800\n",
      "78/78 - 0s - loss: 6.6789 - val_loss: 6.5436 - 132ms/epoch - 2ms/step\n",
      "Epoch 730/800\n",
      "78/78 - 0s - loss: 6.6673 - val_loss: 6.5808 - 144ms/epoch - 2ms/step\n",
      "Epoch 731/800\n",
      "78/78 - 0s - loss: 6.7023 - val_loss: 6.5495 - 146ms/epoch - 2ms/step\n",
      "Epoch 732/800\n",
      "78/78 - 0s - loss: 6.7085 - val_loss: 6.5087 - 147ms/epoch - 2ms/step\n",
      "Epoch 733/800\n",
      "78/78 - 0s - loss: 6.6703 - val_loss: 6.5564 - 150ms/epoch - 2ms/step\n",
      "Epoch 734/800\n",
      "78/78 - 0s - loss: 6.6729 - val_loss: 6.5808 - 145ms/epoch - 2ms/step\n",
      "Epoch 735/800\n",
      "78/78 - 0s - loss: 6.7055 - val_loss: 6.5865 - 145ms/epoch - 2ms/step\n",
      "Epoch 736/800\n",
      "78/78 - 0s - loss: 6.6994 - val_loss: 6.5679 - 152ms/epoch - 2ms/step\n",
      "Epoch 737/800\n",
      "78/78 - 0s - loss: 6.6723 - val_loss: 6.4886 - 137ms/epoch - 2ms/step\n",
      "Epoch 738/800\n",
      "78/78 - 0s - loss: 6.6460 - val_loss: 6.5219 - 140ms/epoch - 2ms/step\n",
      "Epoch 739/800\n",
      "78/78 - 0s - loss: 6.6494 - val_loss: 6.4880 - 145ms/epoch - 2ms/step\n",
      "Epoch 740/800\n",
      "78/78 - 0s - loss: 6.6231 - val_loss: 6.4740 - 146ms/epoch - 2ms/step\n",
      "Epoch 741/800\n",
      "78/78 - 0s - loss: 6.6230 - val_loss: 6.4726 - 121ms/epoch - 2ms/step\n",
      "Epoch 742/800\n",
      "78/78 - 0s - loss: 6.6609 - val_loss: 6.5797 - 118ms/epoch - 2ms/step\n",
      "Epoch 743/800\n",
      "78/78 - 0s - loss: 6.6968 - val_loss: 6.5612 - 120ms/epoch - 2ms/step\n",
      "Epoch 744/800\n",
      "78/78 - 0s - loss: 6.6972 - val_loss: 6.5424 - 119ms/epoch - 2ms/step\n",
      "Epoch 745/800\n",
      "78/78 - 0s - loss: 6.7204 - val_loss: 6.5586 - 116ms/epoch - 1ms/step\n",
      "Epoch 746/800\n",
      "78/78 - 0s - loss: 6.7110 - val_loss: 6.5742 - 122ms/epoch - 2ms/step\n",
      "Epoch 747/800\n",
      "78/78 - 0s - loss: 6.6674 - val_loss: 6.5511 - 122ms/epoch - 2ms/step\n",
      "Epoch 748/800\n",
      "78/78 - 0s - loss: 6.6835 - val_loss: 6.5335 - 122ms/epoch - 2ms/step\n",
      "Epoch 749/800\n",
      "78/78 - 0s - loss: 6.6565 - val_loss: 6.5383 - 117ms/epoch - 2ms/step\n",
      "Epoch 750/800\n",
      "78/78 - 0s - loss: 6.6884 - val_loss: 6.5766 - 116ms/epoch - 1ms/step\n",
      "Epoch 751/800\n",
      "78/78 - 0s - loss: 6.6747 - val_loss: 6.5541 - 117ms/epoch - 1ms/step\n",
      "Epoch 752/800\n",
      "78/78 - 0s - loss: 6.7112 - val_loss: 6.5319 - 121ms/epoch - 2ms/step\n",
      "Epoch 753/800\n",
      "78/78 - 0s - loss: 6.7060 - val_loss: 6.5470 - 117ms/epoch - 2ms/step\n",
      "Epoch 754/800\n",
      "78/78 - 0s - loss: 6.6766 - val_loss: 6.4705 - 119ms/epoch - 2ms/step\n",
      "Epoch 755/800\n",
      "78/78 - 0s - loss: 6.6932 - val_loss: 6.5271 - 121ms/epoch - 2ms/step\n",
      "Epoch 756/800\n",
      "78/78 - 0s - loss: 6.6942 - val_loss: 6.5492 - 124ms/epoch - 2ms/step\n",
      "Epoch 757/800\n",
      "78/78 - 0s - loss: 6.6907 - val_loss: 6.4900 - 124ms/epoch - 2ms/step\n",
      "Epoch 758/800\n",
      "78/78 - 0s - loss: 6.6596 - val_loss: 6.5104 - 123ms/epoch - 2ms/step\n",
      "Epoch 759/800\n",
      "78/78 - 0s - loss: 6.6593 - val_loss: 6.5369 - 124ms/epoch - 2ms/step\n",
      "Epoch 760/800\n",
      "78/78 - 0s - loss: 6.6777 - val_loss: 6.5255 - 122ms/epoch - 2ms/step\n",
      "Epoch 761/800\n",
      "78/78 - 0s - loss: 6.6640 - val_loss: 6.5242 - 122ms/epoch - 2ms/step\n",
      "Epoch 762/800\n",
      "78/78 - 0s - loss: 6.6993 - val_loss: 6.5977 - 122ms/epoch - 2ms/step\n",
      "Epoch 763/800\n",
      "78/78 - 0s - loss: 6.7199 - val_loss: 6.5553 - 121ms/epoch - 2ms/step\n",
      "Epoch 764/800\n",
      "78/78 - 0s - loss: 6.7036 - val_loss: 6.5264 - 120ms/epoch - 2ms/step\n",
      "Epoch 765/800\n",
      "78/78 - 0s - loss: 6.6563 - val_loss: 6.5107 - 119ms/epoch - 2ms/step\n",
      "Epoch 766/800\n",
      "78/78 - 0s - loss: 6.6155 - val_loss: 6.4477 - 116ms/epoch - 1ms/step\n",
      "Epoch 767/800\n",
      "78/78 - 0s - loss: 6.6315 - val_loss: 6.4654 - 120ms/epoch - 2ms/step\n",
      "Epoch 768/800\n",
      "78/78 - 0s - loss: 6.6238 - val_loss: 6.4758 - 116ms/epoch - 1ms/step\n",
      "Epoch 769/800\n",
      "78/78 - 0s - loss: 6.6274 - val_loss: 6.5105 - 121ms/epoch - 2ms/step\n",
      "Epoch 770/800\n",
      "78/78 - 0s - loss: 6.6316 - val_loss: 6.4748 - 118ms/epoch - 2ms/step\n",
      "Epoch 771/800\n",
      "78/78 - 0s - loss: 6.6373 - val_loss: 6.4893 - 120ms/epoch - 2ms/step\n",
      "Epoch 772/800\n",
      "78/78 - 0s - loss: 6.6288 - val_loss: 6.5019 - 118ms/epoch - 2ms/step\n",
      "Epoch 773/800\n",
      "78/78 - 0s - loss: 6.6330 - val_loss: 6.4880 - 117ms/epoch - 2ms/step\n",
      "Epoch 774/800\n",
      "78/78 - 0s - loss: 6.6761 - val_loss: 6.5796 - 120ms/epoch - 2ms/step\n",
      "Epoch 775/800\n",
      "78/78 - 0s - loss: 6.7004 - val_loss: 6.5860 - 118ms/epoch - 2ms/step\n",
      "Epoch 776/800\n",
      "78/78 - 0s - loss: 6.6879 - val_loss: 6.5581 - 117ms/epoch - 2ms/step\n",
      "Epoch 777/800\n",
      "78/78 - 0s - loss: 6.6778 - val_loss: 6.5349 - 116ms/epoch - 1ms/step\n",
      "Epoch 778/800\n",
      "78/78 - 0s - loss: 6.6941 - val_loss: 6.5671 - 121ms/epoch - 2ms/step\n",
      "Epoch 779/800\n",
      "78/78 - 0s - loss: 6.7003 - val_loss: 6.5734 - 118ms/epoch - 2ms/step\n",
      "Epoch 780/800\n",
      "78/78 - 0s - loss: 6.6996 - val_loss: 6.5627 - 118ms/epoch - 2ms/step\n",
      "Epoch 781/800\n",
      "78/78 - 0s - loss: 6.6787 - val_loss: 6.5542 - 124ms/epoch - 2ms/step\n",
      "Epoch 782/800\n",
      "78/78 - 0s - loss: 6.6634 - val_loss: 6.4858 - 122ms/epoch - 2ms/step\n",
      "Epoch 783/800\n",
      "78/78 - 0s - loss: 6.6375 - val_loss: 6.4869 - 146ms/epoch - 2ms/step\n",
      "Epoch 784/800\n",
      "78/78 - 0s - loss: 6.6384 - val_loss: 6.4704 - 148ms/epoch - 2ms/step\n",
      "Epoch 785/800\n",
      "78/78 - 0s - loss: 6.6645 - val_loss: 6.5219 - 136ms/epoch - 2ms/step\n",
      "Epoch 786/800\n",
      "78/78 - 0s - loss: 6.6714 - val_loss: 6.5138 - 118ms/epoch - 2ms/step\n",
      "Epoch 787/800\n",
      "78/78 - 0s - loss: 6.6577 - val_loss: 6.5209 - 121ms/epoch - 2ms/step\n",
      "Epoch 788/800\n",
      "78/78 - 0s - loss: 6.6933 - val_loss: 6.5681 - 117ms/epoch - 1ms/step\n",
      "Epoch 789/800\n",
      "78/78 - 0s - loss: 6.6927 - val_loss: 6.5381 - 119ms/epoch - 2ms/step\n",
      "Epoch 790/800\n",
      "78/78 - 0s - loss: 6.7181 - val_loss: 6.5942 - 119ms/epoch - 2ms/step\n",
      "Epoch 791/800\n",
      "78/78 - 0s - loss: 6.6607 - val_loss: 6.5307 - 118ms/epoch - 2ms/step\n",
      "Epoch 792/800\n",
      "78/78 - 0s - loss: 6.6669 - val_loss: 6.6077 - 119ms/epoch - 2ms/step\n",
      "Epoch 793/800\n",
      "78/78 - 0s - loss: 6.6566 - val_loss: 6.4970 - 117ms/epoch - 1ms/step\n",
      "Epoch 794/800\n",
      "78/78 - 0s - loss: 6.6499 - val_loss: 6.4690 - 116ms/epoch - 1ms/step\n",
      "Epoch 795/800\n",
      "78/78 - 0s - loss: 6.6088 - val_loss: 6.4648 - 118ms/epoch - 2ms/step\n",
      "Epoch 796/800\n",
      "78/78 - 0s - loss: 6.6013 - val_loss: 6.5096 - 118ms/epoch - 2ms/step\n",
      "Epoch 797/800\n",
      "78/78 - 0s - loss: 6.6322 - val_loss: 6.4927 - 120ms/epoch - 2ms/step\n",
      "Epoch 798/800\n",
      "78/78 - 0s - loss: 6.6325 - val_loss: 6.5093 - 119ms/epoch - 2ms/step\n",
      "Epoch 799/800\n",
      "78/78 - 0s - loss: 6.6604 - val_loss: 6.5344 - 117ms/epoch - 2ms/step\n",
      "Epoch 800/800\n",
      "78/78 - 0s - loss: 6.6753 - val_loss: 6.5775 - 119ms/epoch - 2ms/step\n"
     ]
    }
   ],
   "source": [
    "train_perticular_model(models[0], -1, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Training Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Custom_Layer(layers.Layer):\n",
    "    def __init__(self, weights, bias=False):\n",
    "        super(Custom_Layer, self).__init__()\n",
    "        self.w = weights\n",
    "        if bias:\n",
    "            self.b = bias\n",
    "\n",
    "    def call(self, inputs):\n",
    "        try: return matmul(inputs, self.w) + self.b\n",
    "        except: return matmul(inputs, self.w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, [], None, [], [], None]\n"
     ]
    }
   ],
   "source": [
    "print([[] if i in [1, 3, 4] else None for i in range(6)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_and_discard(indexes):\n",
    "    final_models = [models[i][-1] if i in indexes else None for i in range(6)]\n",
    "    std = [[] if i in indexes else None for i in range(6)]\n",
    "    for i, model in enumerate(final_models):\n",
    "        if model is not None:\n",
    "            for layer in model.layers:\n",
    "                try: std[i].append(np.array(math.reduce_std(layer.weights[0])) * 2)\n",
    "                except: std[i].append([])\n",
    "\n",
    "    for index, model in enumerate(final_models):\n",
    "        if model is not None:\n",
    "            final_layers = Sequential([Input(shape=(None, 324))])\n",
    "            for i, layer in enumerate(model.layers):\n",
    "                try:\n",
    "                    temp_layer = []\n",
    "                    for j in layer.weights[0]:\n",
    "                        temp_weights = []\n",
    "                        for k in j:\n",
    "                            if k < std[index][i]: temp_weights.append(0)\n",
    "                            else:          temp_weights.append(1)  \n",
    "                        temp_layer.append(temp_weights)\n",
    "                    weights = Variable(temp_layer, dtype='float32')\n",
    "                    try: final_layers.add(Custom_Layer(weights, bias=layer.weights[1]))\n",
    "                    except: final_layers.add(Custom_Layer(weights))\n",
    "                except IndexError: final_layers.add(layer)\n",
    "\n",
    "            final_layers.compile(optimizer='adam', loss='mse')\n",
    "            final_layers.build(input_shape=(None, 324))\n",
    "            # print(final_layers.summary())\n",
    "\n",
    "            final_models[index] = final_layers\n",
    "        \n",
    "    return final_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = [0]\n",
    "final_models = std_and_discard(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_models = [[] if i in indexes else None for i in range(6)]\n",
    "for i in range(6):\n",
    "    if predictor_models[i] is not None:\n",
    "        for j in [3, 5, 7]:\n",
    "            predictor_models[i].append(Model(inputs=final_models[i].input, outputs=final_models[i].layers[j].output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking Co-relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature(data):\n",
    "  new_data = []\n",
    "  for i in range(len(data)):\n",
    "    d = data.iloc[i]\n",
    "    new_data.append(list(d[1:]))\n",
    "  return np.array(new_data)\n",
    "\n",
    "def predictor(month,mod):\n",
    "  month_path = slp_months[month]\n",
    "  month_data = pd.read_csv(month_path)\n",
    "  feature = get_feature(month_data)\n",
    "  try: pred_m = models[mod](feature)\n",
    "  except: \n",
    "    print(i)\n",
    "    raise(IndexError)\n",
    "  pred_f = pred_m.numpy()\n",
    "  return pred_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def get_top_pred(top,pred_info,w_start,w_end):\n",
    "    pred_f = pred_info\n",
    "    cor_list = []\n",
    "    score = []\n",
    "    \n",
    "    for j in range(w_start,w_end):\n",
    "      score.append(pred_f[j][i])\n",
    "    corr, _ = pearsonr(rain_fall_data[w_start:w_end],score)\n",
    "    cor_list.append(corr)\n",
    "    list1=list(enumerate(cor_list))\n",
    "    list2=sorted(list1, key=lambda x: x[1],reverse=True)\n",
    "    top_feature_index = []\n",
    "\n",
    "    for i in range(top):\n",
    "        index = list2[i][0]\n",
    "        top_feature_index.append(index)\n",
    "\n",
    "    predictor = []\n",
    "    for i in range(len(pred_f)):\n",
    "        temp = []\n",
    "        for j in top_feature_index:\n",
    "            feature = pred_f[i][j]\n",
    "            temp.append(feature)\n",
    "            predictor.append(temp)\n",
    "    \n",
    "    return predictor\n",
    "\n",
    "\n",
    "def get_predictors(months,top,w_start,w_end):\n",
    "  for i in months:\n",
    "    for mod in range(len(models)):\n",
    "      y_pred = predictor(i,mod)\n",
    "      b = get_top_pred(top, y_pred, w_start,w_end)\n",
    "      try:    pred = np.concatenate((pred,b), axis=1)\n",
    "      except: pred = np.array(b)\n",
    "  return pred\n",
    "  \n",
    "\n",
    "def window_solution(months,top):\n",
    "  reg = SVR(kernel = 'rbf',C=1.0,epsilon=0.45)\n",
    "  k=0\n",
    "  cor_all = []\n",
    "  for window in range(10,53):\n",
    "    k+=1\n",
    "    score = []\n",
    "    pred = get_predictors(months,top,53-window,53)\n",
    "    for i in range(14):\n",
    "      reg.fit(pred[53-window:53+i], rain_fall_data[53-window:53+i])\n",
    "      \n",
    "      score.append(reg.predict([pred[53+i]])[0])\n",
    "    corr, _ = pearsonr(rain_fall_data[53:67],score)\n",
    "    print(\"Window size = \",window,\"   plcc\",corr)\n",
    "    cor_all.append(corr)\n",
    "  return min(cor_all),max(cor_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from itertools import combinations\n",
    "# months_comb = [j for i in range(4) for j in combinations(range(5,9), i+1)]\n",
    "months_comb = [[4]]\n",
    "top = 9\n",
    "\n",
    "\n",
    "\n",
    "minimum, maximum = 1, -1\n",
    "for month in months_comb:\n",
    "  for i in range(1,top):\n",
    "    left, right = window_solution(month,i)\n",
    "    print(\"\\n\\nMonth combination = \",month,\"    top = \",i,\"   min and max\",left,\"   \",right)\n",
    "    if minimum > left:  minimum, min_month, min_top = left, month, i\n",
    "    if maximum < right: maximum, max_month, max_top = right, month, i\n",
    "    print(f'Minimum: {minimum}, {min_month}, {min_top}    Maximum: {maximum}, {max_month}, {max_top}\\n')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b7f7c4ea1473e71f2b02dcfd9c6eeea46e4b7e6158f129dcc91468de36ded5f2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
